<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>干点啥好呢</title><meta name="author" content="Me"><meta name="copyright" content="Me"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="干点啥好呢">
<meta property="og:url" content="http://example.com/page/2/">
<meta property="og:site_name" content="干点啥好呢">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg">
<meta property="article:author" content="Me">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '干点啥好呢',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-09-12 21:36:38'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr class="custom-hr"/></div></div><div class="page" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="干点啥好呢"><span class="site-name">干点啥好呢</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/22/PyTorch-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" title="PyTorch:损失函数">PyTorch:损失函数</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-22T04:19:16.000Z" title="发表于 2024-08-22 12:19:16">2024-08-22</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">均方误差MSE
MSE=1N∑i=1N(xi−yi)2MSE=\frac{1}{N}\sum_{i=1}^{N}(x_i-y_i)^2
MSE=N1​i=1∑N​(xi​−yi​)2
函数原型
1class torch.nn.MSELoss(size_average=None, reduce=None, reduction=&#x27;mean&#x27;)
属性说明

reduction

= ‘none’：no reduction will be applied.下面的实例中，在loss1中是按照原始维度输出，即对应位置的元素相减然后求平方
= ‘mean’（默认）：the sum of the output will be divided by the number of elements in the output.
= ‘sum’：the output will be summed.




Note:
size_average and reduce are in the process of being deprecated, and in the meantime, sp ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/22/PyTorch-%E4%BC%98%E5%8C%96%E5%99%A8/" title="PyTorch:优化器">PyTorch:优化器</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-22T04:18:45.000Z" title="发表于 2024-08-22 12:18:45">2024-08-22</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">随机梯度下降SGD

每次更新的时候使用一个样本进行梯度下降，所谓的随机二字，就是说我们可以随机用一个样本来表示所有的样本，来调整超参数。

12import torch.optim as optimoptimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)
momentum

如图所示，红色为SGD+Momentum。黑色为SGD。可以看到黑色为典型Hessian矩阵病态的情况，相当于大幅度的徘徊着向最低点前进。


由于动量积攒了历史的梯度，如点P前一刻的梯度与当前的梯度方向几乎相反。因此原本在P点原本要大幅徘徊的梯度，主要受到前一时刻的影响，而导致在当前时刻的梯度幅度减小。
直观上讲就是，要是当前时刻的梯度与历史时刻梯度方向相似，这种趋势在当前时刻则会加强；要是不同，则当前时刻的梯度方向减弱。
梯度更新公式变化如下：VtV_tVt​可以想象成“方向速度”，与上一次的更新有关。如果上一次梯度与此次方向相同，则∣Vt∣|V_t|∣Vt​∣会越来越大，WWW的更新也越开越快；反之∣Vt∣|V_t|∣Vt​∣变小， ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/22/PyTorch-%E6%95%B0%E6%8D%AE%E9%9B%86-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" title="PyTorch:数据集/数据处理">PyTorch:数据集/数据处理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-22T04:17:58.000Z" title="发表于 2024-08-22 12:17:58">2024-08-22</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">torchvision.transforms.Compose
PyTorch 的 torchvision 库中的一个类，用于将一系列图像变换操作组合在一起。通过将多个图像变换按顺序应用，可以方便地对数据集进行预处理和增强。
原型
1class torchvision.transforms.Compose(transforms)
属性说明

transforms：包含多个变换操作的列表。这些变换操作会按照提供的顺序依次应用到图像上。

主要方法
实例
123456789101112131415161718from torchvision import transformsfrom PIL import Image# 定义一系列变换操作transform = transforms.Compose([    transforms.Resize((128, 128)),  # 调整图像大小    transforms.RandomHorizontalFlip(),  # 随机水平翻转    transforms.ToTensor(),  # 转换为张量    transforms.Normal ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/22/torch-nn-Tutorial/" title="torch.nn Tutorial">torch.nn Tutorial</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-22T04:14:07.000Z" title="发表于 2024-08-22 12:14:07">2024-08-22</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">torch.nn用法
nn.Linear

函数原型
123class nn.Linear(in_features,                 out_features,                 bias = True)

参数含义

in_features：输入张量的长度
out_features：输出张量的长度
bias：是否包含偏置量


原理
Yn×o=Xn×iWi×o+bY_{n×o}=X_{n×i}W_{i×o}+b
Yn×o​=Xn×i​Wi×o​+b
WWW为模型要学习的参数，bbb为偏置向量，nnn为输出向量的行数（例如，你想一次输入10个样本，即 batch_sizebatch\_sizebatch_size 为10，则nnn=10），iii为输入神经元的个数（例如，输入的样本特征数为5，则iii=5），ooo为输出神经元的个数。


实例
假设我们的一次输入三个样本A,B,C（即batch_size为3），每个样本的特征数量为5：
123A: [0.1,0.2,0.3,0.3,0.3]B: [0.4,0.5,0.6,0.6,0.6]C: [0.7,0 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/24/Review-of-Linear-Algebra/" title="Review of Linear Algebra">Review of Linear Algebra</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-24T08:10:06.000Z" title="发表于 2024-06-24 16:10:06">2024-06-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Math/">Math</a></span></div><div class="content">计算非线性函数的 Hessian 矩阵
现有函数 f(w)=w12+2w22+3w32f(w)=w_1^2+2w_2^2+3w_3^2f(w)=w12​+2w22​+3w32​，其中 w=[w1,w2,w3]Tw=[w_1,w_2,w_3]^Tw=[w1​,w2​,w3​]T。


计算一阶导数（梯度）
首先对每个变量 wiw_iwi​ 求一阶偏导数：
∂f∂w1=2w1∂f∂w2=4w2∂f∂w3=6w3\frac{\partial f}{\partial w_1}=2w_1 \\
\frac{\partial f}{\partial w_2}=4w_2 \\
\frac{\partial f}{\partial w_3}=6w_3
∂w1​∂f​=2w1​∂w2​∂f​=4w2​∂w3​∂f​=6w3​
因此，梯度向量 ∇f(w)=(w21,4w2,6w3)T\nabla f(w) = (w2_1,4w_2,6w_3)^T∇f(w)=(w21​,4w2​,6w3​)T 。


计算二阶导数： Hessian 矩阵
矩阵元素 HijH_{ij}Hij​ 是 fff 对 wiw_iwi ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/24/Review-of-Advanced-Mathematics/" title="Review of Advanced Mathematics">Review of Advanced Mathematics</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-24T08:07:55.000Z" title="发表于 2024-06-24 16:07:55">2024-06-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Math/">Math</a></span></div><div class="content">泰勒定理
对于一般的函数，泰勒公式的系数的选择依赖于函数在一点的各阶导数值。
f(a+h)=f(a)+f‘(a)h+o(h)f(a+h) = f(a) + f^`(a)h +o(h)
f(a+h)=f(a)+f‘(a)h+o(h)
其中 o(h)o(h)o(h) 是比 hhh 高阶的无穷小。也即 f(a+h)≈f(a)+f‘(a)hf(a+h) \approx f(a) + f^`(a)hf(a+h)≈f(a)+f‘(a)h 。
f(x)≈f(a)+f‘(a)(x−a)f(x) \approx f(a) + f^`(a)(x-a)f(x)≈f(a)+f‘(a)(x−a) 。
综上，可以将定理描述为：

设 nnn 是一个正整数，如果定义在一个包含 aaa 的区间上的函数 fff 在 aaa 点处 n+1n+1n+1 阶可导，那么对于这个区间上的任意 xxx ，都有：
f(x)=f(a)+f‘(a)1!(x−a)+f(2)(a)2!(x−a)2+⋯+f(n)(a)n!(x−a)n+Rn(x)f(x) = f(a) + \frac{f^`(a)}{1!}(x-a) + \frac{f^{( ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/17/Principles-of-Large-Scale-hhMachine-Learning-Lecture-4/" title="Principles of Large-Scale Machine Learning [Lecture 4]">Principles of Large-Scale Machine Learning [Lecture 4]</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-17T15:07:56.000Z" title="发表于 2024-06-17 23:07:56">2024-06-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Large-Scale-ML/">Large-Scale ML</a></span></div><div class="content">Lecture 4
Learning with Gradient Descent
回顾：经验风险最小化与梯度下降

为每个预测器分配一个ddd维的参数向量，也即每一个ddd维参数向量对应一个预测器。并将经验风险最小化视作一个优化问题：
minimize:R(hw)=1n∑i=1nL(hw(xi),yi)  over  w∈Rd\text{minimize}:R(h_w)=\frac{1}{n}\sum_{i=1}^nL(h_w(x_i),y_i) \; \text{over} \; w \in \mathbb{R}^d
minimize:R(hw​)=n1​i=1∑n​L(hw​(xi​),yi​)overw∈Rd
通常，我们将经验风险视作关于参数www的函数：
f(w)=R(hw)=1n∑i=1nL(hw(xi),yi)=1n∑i=1nfi(w)f(w)=R(h_w)=\frac{1}{n}\sum_{i=1}^nL(h_w(x_i),y_i)=\frac{1}{n}\sum_{i=1}^nf_i(w)
f(w)=R(hw​)=n1​i=1∑n​L(hw​(xi​),yi​)=n1​ ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/16/cifar10-tutorial/" title="CIFAR10 Tutorial">CIFAR10 Tutorial</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-16T14:27:32.000Z" title="发表于 2024-06-16 22:27:32">2024-06-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">123# For tips on running notebooks in Google Colab, see# https://pytorch.org/tutorials/beginner/colab%matplotlib inline
Training a Classifier
This is it. You have seen how to define neural networks, compute loss and make updates to the weights of the network.
已经了解了如何定义神经网络、计算损失以及更新网络权重。
Now you might be thinking, What about data?如何处理不同的数据类型

Generally, when you have to deal with image, text, audio or video data, you can use standard python packages that load data into a numpy array. Then you can ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/14/Principles-of-Large-Scale-hhMachine-Learning-Lecture-3/" title="Principles of Large-Scale Machine Learning [Lecture 3]">Principles of Large-Scale Machine Learning [Lecture 3]</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-14T10:00:18.000Z" title="发表于 2024-06-14 18:00:18">2024-06-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Large-Scale-ML/">Large-Scale ML</a></span></div><div class="content">Lecture 3
Exponential Concentration Inequalities and ERM
回顾：切比雪夫不等式


0−10-10−1经验风险：

R(h)=1n∑i=1nL(h(xi),yi)=1n∑i=1nδ(h(xi),yi)δ(y^,y)=1, if   y^=y  and  0  otherwiseR(h)=\frac{1}{n}\sum_{i=1}^{n}L(h(x_i),y_i)=\frac{1}{n}\sum_{i=1}^n\delta(h(x_i),y_i)\\
\delta(\hat{y},y)=1,\,\text{if}\,\,\,\hat{y}=y\,\,\text{and}\,\,\text{0}\,\,\text{otherwise}
R(h)=n1​i=1∑n​L(h(xi​),yi​)=n1​i=1∑n​δ(h(xi​),yi​)δ(y^​,y)=1,ify^​=yand0otherwise

ZZZ：等概率从所有样本中抽取其损失，进行KKK次彼此独立的随机抽取得到一组随机变量ZZZ，即Z1,Z2,…,ZKZ_1,Z_2,\dot ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/14/Review-of-Statistics/" title="Review of Statistics">Review of Statistics</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-14T09:58:10.000Z" title="发表于 2024-06-14 17:58:10">2024-06-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Math/">Math</a></span></div><div class="content">子采样的数学期望、方差推导
假设ZZZ的期望和方差分别为μ\muμ、σ2\sigma^2σ2，由于Z1,Z2,…,ZKZ_1,Z_2,\dots,Z_KZ1​,Z2​,…,ZK​是独立同分布的随机变量，每个ZkZ_kZk​的期望和方差也是μ\muμ、σ2\sigma^2σ2。SKS_KSK​的定义如下：
SK=1K∑k=1KZkS_K = \frac{1}{K} \sum_{k=1}^K Z_k
SK​=K1​k=1∑K​Zk​


E(SK)\textbf{E}(S_K)E(SK​)
E(SK)=E(1K∑k=1KZk)=1K∑k=1KE(Zk)=1K∑k=1Kμ=μ\textbf{E}(S_K) = \textbf{E}(\frac{1}{K} \sum_{k=1}^K Z_k) = \frac{1}{K}\sum_{k=1}^K\textbf{E}(Z_k)=\frac{1}{K}\sum_{k=1}^K\mu=\mu
E(SK​)=E(K1​k=1∑K​Zk​)=K1​k=1∑K​E(Zk​)=K1​k=1∑K​μ=μ


Var(SK)\textbf{Var}(S_K)Var ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Me</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget self_memo" id="self_memo"><div class="item-headline"><i class="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/microsoft_todo_2019_240px.png"></i><span>TodoList</span></div><div class="item-content"><head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>none</title> <style> body, html { margin: 0; padding: 0; width: 100%; height: 100%; } textarea { width: 100%; height: 100%; box-sizing: border-box; /* 包括内边距和边框在内的宽度和高度计算 */ } </style> </head> <body> <textarea placeholder="今天干什么？"></textarea> </body></div></div><div class="sticky_layout"><div class="card-widget user_map" id="user_map"><div class="item-headline"><i></i><span>Visitors</span></div><div class="item-content"><script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=5V2tOKp8qAdRM-i8eu7ETTO9ugt5uKbbG-U7Yj8uMl8"></script></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By Me</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">理论是灰色的，而生活之树长青。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>