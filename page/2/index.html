<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>干点啥好呢</title><meta name="author" content="Me"><meta name="copyright" content="Me"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="干点啥好呢">
<meta property="og:url" content="http://example.com/page/2/">
<meta property="og:site_name" content="干点啥好呢">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg">
<meta property="article:author" content="Me">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/page/2/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '干点啥好呢',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-09-07 21:55:58'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/></div></div><div class="page" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="干点啥好呢"><span class="site-name">干点啥好呢</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/24/Review-of-Advanced-Mathematics/" title="Review of Advanced Mathematics">Review of Advanced Mathematics</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-24T08:07:55.000Z" title="发表于 2024-06-24 16:07:55">2024-06-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Math/">Math</a></span></div><div class="content">泰勒定理
对于一般的函数，泰勒公式的系数的选择依赖于函数在一点的各阶导数值。
f(a+h)=f(a)+f‘(a)h+o(h)f(a+h) = f(a) + f^`(a)h +o(h)
f(a+h)=f(a)+f‘(a)h+o(h)
其中 o(h)o(h)o(h) 是比 hhh 高阶的无穷小。也即 f(a+h)≈f(a)+f‘(a)hf(a+h) \approx f(a) + f^`(a)hf(a+h)≈f(a)+f‘(a)h 。
f(x)≈f(a)+f‘(a)(x−a)f(x) \approx f(a) + f^`(a)(x-a)f(x)≈f(a)+f‘(a)(x−a) 。
综上，可以将定理描述为：

设 nnn 是一个正整数，如果定义在一个包含 aaa 的区间上的函数 fff 在 aaa 点处 n+1n+1n+1 阶可导，那么对于这个区间上的任意 xxx ，都有：
f(x)=f(a)+f‘(a)1!(x−a)+f(2)(a)2!(x−a)2+⋯+f(n)(a)n!(x−a)n+Rn(x)f(x) = f(a) + \frac{f^`(a)}{1!}(x-a) + \frac{f^{( ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/17/Principles-of-Large-Scale-hhMachine-Learning-Lecture-4/" title="Principles of Large-Scale Machine Learning [Lecture 4]">Principles of Large-Scale Machine Learning [Lecture 4]</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-17T15:07:56.000Z" title="发表于 2024-06-17 23:07:56">2024-06-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Large-Scale-ML/">Large-Scale ML</a></span></div><div class="content">Lecture 4
Learning with Gradient Descent
回顾：经验风险最小化与梯度下降

为每个预测器分配一个ddd维的参数向量，也即每一个ddd维参数向量对应一个预测器。并将经验风险最小化视作一个优化问题：
minimize:R(hw)=1n∑i=1nL(hw(xi),yi)  over  w∈Rd\text{minimize}:R(h_w)=\frac{1}{n}\sum_{i=1}^nL(h_w(x_i),y_i) \; \text{over} \; w \in \mathbb{R}^d
minimize:R(hw​)=n1​i=1∑n​L(hw​(xi​),yi​)overw∈Rd
通常，我们将经验风险视作关于参数www的函数：
f(w)=R(hw)=1n∑i=1nL(hw(xi),yi)=1n∑i=1nfi(w)f(w)=R(h_w)=\frac{1}{n}\sum_{i=1}^nL(h_w(x_i),y_i)=\frac{1}{n}\sum_{i=1}^nf_i(w)
f(w)=R(hw​)=n1​i=1∑n​L(hw​(xi​),yi​)=n1​ ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/16/cifar10-tutorial/" title="CIFAR10 Tutorial">CIFAR10 Tutorial</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-16T14:27:32.000Z" title="发表于 2024-06-16 22:27:32">2024-06-16</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">123# For tips on running notebooks in Google Colab, see# https://pytorch.org/tutorials/beginner/colab%matplotlib inline
Training a Classifier
This is it. You have seen how to define neural networks, compute loss and make updates to the weights of the network.
已经了解了如何定义神经网络、计算损失以及更新网络权重。
Now you might be thinking, What about data?如何处理不同的数据类型

Generally, when you have to deal with image, text, audio or video data, you can use standard python packages that load data into a numpy array. Then you can ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/14/Principles-of-Large-Scale-hhMachine-Learning-Lecture-3/" title="Principles of Large-Scale Machine Learning [Lecture 3]">Principles of Large-Scale Machine Learning [Lecture 3]</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-14T10:00:18.000Z" title="发表于 2024-06-14 18:00:18">2024-06-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Large-Scale-ML/">Large-Scale ML</a></span></div><div class="content">Lecture 3
Exponential Concentration Inequalities and ERM
回顾：切比雪夫不等式


0−10-10−1经验风险：

R(h)=1n∑i=1nL(h(xi),yi)=1n∑i=1nδ(h(xi),yi)δ(y^,y)=1, if   y^=y  and  0  otherwiseR(h)=\frac{1}{n}\sum_{i=1}^{n}L(h(x_i),y_i)=\frac{1}{n}\sum_{i=1}^n\delta(h(x_i),y_i)\\
\delta(\hat{y},y)=1,\,\text{if}\,\,\,\hat{y}=y\,\,\text{and}\,\,\text{0}\,\,\text{otherwise}
R(h)=n1​i=1∑n​L(h(xi​),yi​)=n1​i=1∑n​δ(h(xi​),yi​)δ(y^​,y)=1,ify^​=yand0otherwise

ZZZ：等概率从所有样本中抽取其损失，进行KKK次彼此独立的随机抽取得到一组随机变量ZZZ，即Z1,Z2,…,ZKZ_1,Z_2,\dot ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/14/Review-of-Statistics/" title="Review of Statistics">Review of Statistics</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-14T09:58:10.000Z" title="发表于 2024-06-14 17:58:10">2024-06-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Math/">Math</a></span></div><div class="content">子采样的数学期望、方差推导
假设ZZZ的期望和方差分别为μ\muμ、σ2\sigma^2σ2，由于Z1,Z2,…,ZKZ_1,Z_2,\dots,Z_KZ1​,Z2​,…,ZK​是独立同分布的随机变量，每个ZkZ_kZk​的期望和方差也是μ\muμ、σ2\sigma^2σ2。SKS_KSK​的定义如下：
SK=1K∑k=1KZkS_K = \frac{1}{K} \sum_{k=1}^K Z_k
SK​=K1​k=1∑K​Zk​


E(SK)\textbf{E}(S_K)E(SK​)
E(SK)=E(1K∑k=1KZk)=1K∑k=1KE(Zk)=1K∑k=1Kμ=μ\textbf{E}(S_K) = \textbf{E}(\frac{1}{K} \sum_{k=1}^K Z_k) = \frac{1}{K}\sum_{k=1}^K\textbf{E}(Z_k)=\frac{1}{K}\sum_{k=1}^K\mu=\mu
E(SK​)=E(K1​k=1∑K​Zk​)=K1​k=1∑K​E(Zk​)=K1​k=1∑K​μ=μ


Var(SK)\textbf{Var}(S_K)Var ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/12/neural-networks-tutorial/" title="Neural Networks Tutorial">Neural Networks Tutorial</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-12T11:41:57.000Z" title="发表于 2024-06-12 19:41:57">2024-06-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">123# For tips on running notebooks in Google Colab, see# https://pytorch.org/tutorials/beginner/colab%matplotlib inline
Neural Networks
Neural networks can be constructed using the torch.nn package.
Now that you had a glimpse of autograd, nn depends on autograd to define models and differentiate them. An nn.Module contains layers, and a method forward(input) that returns the output.
For example, look at this network that classifies digit images:

It is a simple feed-forward network. It takes the ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/12/autograd-tutorial/" title="Autograd Tutorial">Autograd Tutorial</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-12T11:28:05.000Z" title="发表于 2024-06-12 19:28:05">2024-06-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">123# For tips on running notebooks in Google Colab, see# https://pytorch.org/tutorials/beginner/colab%matplotlib inline
A Gentle Introduction to torch.autograd
torch.autograd is PyTorch’s automatic differentiation engine that powers neural network training. In this section, you will get a conceptual understanding of how autograd helps a neural network train.
Background：神经网络
Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are define ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/12/tensor-tutorial/" title="Tensor Tutorial">Tensor Tutorial</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-12T10:42:42.000Z" title="发表于 2024-06-12 18:42:42">2024-06-12</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="content">123# For tips on running notebooks in Google Colab, see# https://pytorch.org/tutorials/beginner/colab%matplotlib inline
Tensors
Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.
Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing. If you’re familiar with ndarrays, you’ll be ri ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/11/%E5%91%A8%E6%8A%A5%E6%B1%87%E6%80%BB/" title="周报汇总（持续更新）">周报汇总（持续更新）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-11T15:30:31.000Z" title="发表于 2024-06-11 23:30:31">2024-06-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Learning-Record/">Learning-Record</a></span></div><div class="content">Week 1
0608
Principles of Large-Scale Machine Learning [Lecture1、2]

Estimating the empirical risk with samples


Empirical Risk介绍


计算加速措施：并行化、小批量梯度下降、硬件加速、近似计算


近似计算Empirical Risk的原理


子采样
引入随机变量 Z，代表随机采样一个样本的损失值。多次独立地抽取 Z 的样本，这些样本的平均值将近似于经验风险 R~emp~。


大数定律
根据统计学原理，一组独立随机变量的平均值往往聚集在该随机变量的期望值周围。


中心极限定理
随着我们采样更多的 Z，样本平均值会越来越接近真实的期望值。


集中不等式
随机变量 Z 集中在某个取值附近的概率：马尔可夫不等式、切比雪夫不等式、霍夫丁不等式。





0610
PyTorch


Tensor Tutorial


初始化
属性：shape、dtype、device等
操作：转置、索引、切分等


Autograd Tutorial


神经网络简介：前 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/06/11/CNN-1/" title="CNN(1)">CNN(1)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-06-11T14:48:32.000Z" title="发表于 2024-06-11 22:48:32">2024-06-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Basic-ML/">Basic-ML</a></span></div><div class="content">
多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。然而对于高维感知数据（图像），这种缺少结构的网络可能会变得不实用。

图像处理的问题


图像需要处理的数据量太大，导致成本很高，效率很低

图像是由像素构成的，每个像素又是由颜色构成的。如果一张图片像素为1000×1000，每个像素有RGB三个信息表示颜色信息，则一张图片需要处理三百万个参数。


**CNN 解决的第一个问题就是「将复杂问题简化」，把大量参数降维成少量参数，再做处理。**我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。



图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高


假如有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，图像的内容（本质）并没有发生变化，只是位置发生了变化。


CNN用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。



人类视觉原理

从原始信号摄入开始（瞳孔摄入像素 Pi ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/#content-inner">3</a><a class="extend next" rel="next" href="/page/3/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Me</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">23</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget self_memo" id="self_memo"><div class="item-headline"><i class="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/microsoft_todo_2019_240px.png"></i><span>TodoList</span></div><div class="item-content"><head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>none</title> <style> body, html { margin: 0; padding: 0; width: 100%; height: 100%; } textarea { width: 100%; height: 100%; box-sizing: border-box; /* 包括内边距和边框在内的宽度和高度计算 */ } </style> </head> <body> <textarea placeholder="今天干什么？"></textarea> </body></div></div><div class="sticky_layout"><div class="card-widget user_map" id="user_map"><div class="item-headline"><i></i><span>Visitors</span></div><div class="item-content"><script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=5V2tOKp8qAdRM-i8eu7ETTO9ugt5uKbbG-U7Yj8uMl8"></script></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By Me</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">理论是灰色的，而生活之树长青。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>