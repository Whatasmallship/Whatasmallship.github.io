<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>neural_networks_tutorial</title>
      <link href="/2024/06/12/neural-networks-tutorial/"/>
      <url>/2024/06/12/neural-networks-tutorial/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For tips on running notebooks in Google Colab, see</span></span><br><span class="line"><span class="comment"># https://pytorch.org/tutorials/beginner/colab</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><p>Neural networks can be constructed using the <code>torch.nn</code> package.</p><p>Now that you had a glimpse of <code>autograd</code>, <code>nn</code> depends on <code>autograd</code> to define models and differentiate them. An <code>nn.Module</code> contains layers, and a method <code>forward(input)</code> that returns the <code>output</code>.</p><p>For example, look at this network that classifies digit images:</p><p><img src="https://pytorch.org/tutorials/_static/img/mnist.png" alt="convnet"></p><p>It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.</p><p>A typical training procedure for a neural network is as follows:</p><ul><li>Define the neural network that has some learnable parameters (or weights)定义神经网络</li><li>Iterate over a dataset of inputs在输入数据集上迭代</li><li>Process input through the network通过nn处理输入</li><li>Compute the loss (how far is the output from being correct)计算损失</li><li>Propagate gradients back into the network’s parameters梯度反向传播</li><li>Update the weights of the network, typically using a simple update rule: <code>weight = weight - learning_rate * gradient</code>更新网络参数</li></ul><h2 id="Define-the-network"><a href="#Define-the-network" class="headerlink" title="Define the network"></a>Define the network</h2><p>Let’s define this network:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution；1 个输入图像通道，6 个输出通道，5x5 方形卷积</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)<span class="comment"># 参数含义依次是：输入的通道数目、输出的通道数目、卷积核的大小（当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽）</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b；仿射运算</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)  <span class="comment"># 5*5 from image dimension </span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># Convolution layer C1: 1 input image channel, 6 output channels,卷积层C1：1个输入图像通道，6个输出通道，</span></span><br><span class="line">        <span class="comment"># 5x5 square convolution, it uses RELU activation function, and；5x5方形卷积，它使用RELU激活函数，并且</span></span><br><span class="line">        <span class="comment"># outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch；输出大小为 (N, 6, 28, 28) 的 Tensor，其中 N 是批次的大小</span></span><br><span class="line">        c1 = F.relu(self.conv1(<span class="built_in">input</span>))<span class="comment"># 卷积1-&gt;激活；(N, 6, 28, 28)（在每个输入通道的大小为32x32的情况下）</span></span><br><span class="line">        <span class="comment"># Subsampling layer S2: 2x2 grid, purely functional,</span></span><br><span class="line">        <span class="comment"># this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor</span></span><br><span class="line">        s2 = F.max_pool2d(c1, (<span class="number">2</span>, <span class="number">2</span>))<span class="comment"># 下采样，kernel=2 x 2；(N, 6, 14, 14)</span></span><br><span class="line">        <span class="comment"># Convolution layer C3: 6 input channels, 16 output channels,</span></span><br><span class="line">        <span class="comment"># 5x5 square convolution, it uses RELU activation function, and</span></span><br><span class="line">        <span class="comment"># outputs a (N, 16, 10, 10) Tensor</span></span><br><span class="line">        c3 = F.relu(self.conv2(s2))<span class="comment"># 卷积2-&gt;激活；(N, 16, 10, 10)</span></span><br><span class="line">        <span class="comment"># Subsampling layer S4: 2x2 grid, purely functional,</span></span><br><span class="line">        <span class="comment"># this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor</span></span><br><span class="line">        s4 = F.max_pool2d(c3, <span class="number">2</span>)<span class="comment"># 下采样，kernel=2 x 2；(N, 16, 5, 5)</span></span><br><span class="line">        <span class="comment"># Flatten operation: purely functional, outputs a (N, 400) Tensor</span></span><br><span class="line">        s4 = torch.flatten(s4, <span class="number">1</span>)<span class="comment"># 展平</span></span><br><span class="line">        <span class="comment"># Fully connected layer F5: (N, 400) Tensor input,</span></span><br><span class="line">        <span class="comment"># and outputs a (N, 120) Tensor, it uses RELU activation function</span></span><br><span class="line">        f5 = F.relu(self.fc1(s4))<span class="comment"># 线性层-&gt;激活；(N, 120)</span></span><br><span class="line">        <span class="comment"># Fully connected layer F6: (N, 120) Tensor input,</span></span><br><span class="line">        <span class="comment"># and outputs a (N, 84) Tensor, it uses RELU activation function</span></span><br><span class="line">        f6 = F.relu(self.fc2(f5))<span class="comment"># 线性层-&gt;激活；(N, 84)</span></span><br><span class="line">        <span class="comment"># Gaussian layer OUTPUT: (N, 84) Tensor input, and</span></span><br><span class="line">        <span class="comment"># outputs a (N, 10) Tensor</span></span><br><span class="line">        output = self.fc3(f6)<span class="comment"># 线性层-&gt;激活；(N, 10)</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><pre><code>Net(  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))  (fc1): Linear(in_features=400, out_features=120, bias=True)  (fc2): Linear(in_features=120, out_features=84, bias=True)  (fc3): Linear(in_features=84, out_features=10, bias=True))</code></pre><p>You just have to define the <code>forward</code> function, and the <code>backward</code> function (where gradients are computed) is automatically defined for you using <code>autograd</code>. You can use any of the Tensor operations in the <code>forward</code> function.</p><p>只需要定义 <code>forward</code> 函数， <code>backward</code> 会再使用自动微分时自动执行。</p><p>The learnable parameters of a model are returned by <code>net.parameters()</code></p><p><code>net.parameters()</code> 会返回一个模型中所有可训练的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight：6为输出通道数（有6个不同的卷积核），1为输入通道数，卷积核形状为5x5</span></span><br></pre></td></tr></table></figure><pre><code>10torch.Size([6, 1, 5, 5])</code></pre><p>Let&#39;s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)<span class="comment"># batch_size=1，channels=1，w x h=32x32</span></span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.0659,  0.1170, -0.1006, -0.0936, -0.0479, -0.0695,  0.0936,  0.0952,         -0.0907, -0.0092]], grad_fn=&lt;AddmmBackward0&gt;)</code></pre><p>Zero the gradient buffers of all parameters and backprops with random gradients:</p><p>梯度清空，反向传播梯度（设置一个随机的初始梯度值，该梯度的形状大小需要和out相同）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><div style="background-color: #292D3E; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px"><strong>NOTE:</strong></div><div style="background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px"><p><code>torch.nn</code> only supports mini-batches. The entire <code>torch.nn</code>package only supports inputs that are a mini-batch of samples, and not a single sample.For example, <code>nn.Conv2d</code> will take in a 4D Tensor of<code>n Samples x n Channels x Height x Width</code>.If you have a single sample, just use <code>input.unsqueeze(0)</code> to add a fake batch dimension.</p></div><p>在 PyTorch 中，torch.nn 模块设计用于处理小批量数据（mini-batches），而不是单个样本。这意味着很多神经网络层（如 nn.Conv2d）期望输入是一个四维张量，其形状为 (batch_size, n_channels, height, width)，其中 batch_size 是样本的数量。因此，即使你只有一个样本，你仍然需要提供一个具有批量维度的输入张量。</p><p>input.unsqueeze(0) 的作用是向你的输入张量 input 添加一个新的维度，表示批量大小。这确保了输入符合 torch.nn 模块的预期格式。(如：input &#x3D; torch.randn(3, 32, 32)，input &#x3D; input.unsqueeze(0))</p><p>Before proceeding further, let&#39;s recap all the classes you’ve seen so far.</p><p><strong>Recap:</strong> 回顾</p><ul><li><code>torch.Tensor</code> - A <em>multi-dimensional array</em> with support for autograd operations like <code>backward()</code>. Also <em>holds the gradient</em> w.r.t. the tensor.</li><li><code>nn.Module</code> - Neural network module. <em>Convenient way of encapsulating parameters</em>, with helpers for moving them to GPU, exporting, loading, etc.</li><li><code>nn.Parameter</code> - A kind of Tensor, that is <em>automatically registered as a parameter when assigned as an attribute to a</em>  <code>Module</code>.</li><li><code>autograd.Function</code> - Implements <em>forward and backward definitions of an autograd operation</em>. Every <code>Tensor</code> operation creates at least a single <code>Function</code> node that connects to functions that created a <code>Tensor</code> and <em>encodes its history</em>.</li></ul><p><strong>At this point, we covered:</strong></p><ul><li>Defining a neural network：定义一个神经网络</li><li>Processing inputs and calling backward：处理输入，发起反馈</li></ul><p><strong>Still Left:</strong></p><ul><li>Computing the loss：计算损失</li><li>Updating the weights of the network：更新网络参数</li></ul><h1 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h1><p>A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.</p><p>损失函数采用（输出，目标）输入对，并计算一个值来估计输出与目标的距离。</p><p>There are several different <a href="https://pytorch.org/docs/nn.html#loss-functions">loss functions</a> under the nn package . A simple loss is: <code>nn.MSELoss</code> which computes the mean-squared error between the output and the target.</p><p>nn 包下有几种不同的损失函数。一个简单的损失是：nn.MSELoss，它计算输出和目标之间的均方误差。</p><p>For example:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()<span class="comment"># 设置损失计算标准</span></span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)<span class="comment"># 计算</span></span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(0.6975, grad_fn=&lt;MseLossBackward0&gt;)</code></pre><p>Now, if you follow <code>loss</code> in the backward direction, using its <code>.grad_fn</code> attribute, you will see a graph of computations that looks like this:</p><p>现在，如果使用 <code>loss</code> 的 .grad_fn 属性向后跟踪损失，您将看到如下所示的计算图：</p><figure class="highlight plaintext"><figcaption><span>.sh&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; flatten -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure><p>So, when we call <code>loss.backward()</code>, the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have <code>requires_grad=True</code> will have their <code>.grad</code> Tensor accumulated with the gradient.</p><p>因此，调用 loss.backward() 时，整个计算图相对于神经网络的参数进行求导（微分）。对于计算图中的所有张量，如果其 requires_grad 属性为 True，这些张量的 .grad 属性将累积其梯度。</p><p>For illustration, let us follow a few steps backward:</p><p>loss.grad_fn 是一个与 loss 张量相关的函数，表示计算该张量所涉及的最后一个操作（函数）的梯度函数。具体来说，grad_fn 是一个 Function 对象，它记录了创建该张量的操作，从而使得反向传播过程能够计算梯度。</p><p>next_functions 是一个包含后续操作的元组列表。loss.grad_fn.next_functions[0][0] 指向 MseLossBackward 的第一个输入，即 output 的 grad_fn。next_functions[0]是 MseLossBackward 的第一个输入，继续索引到[0]即其grad_fn成员。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss.grad_fn)  <span class="comment"># MSELoss（生成loss的操作）</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear（生成MSELoss的操作，即MSELoss之前的操作，应该是网络最后一层）</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU（生成Linear的操作，即Linear之前的操作，应该是激活函数）</span></span><br></pre></td></tr></table></figure><pre><code>&lt;MseLossBackward0 object at 0x00000175696E4BB0&gt;&lt;AddmmBackward0 object at 0x00000175696E48B0&gt;&lt;AccumulateGrad object at 0x00000175696E4BB0&gt;</code></pre><h1 id="Backprop"><a href="#Backprop" class="headerlink" title="Backprop"></a>Backprop</h1><p>To backpropagate the error all we have to do is to <code>loss.backward()</code>. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.</p><p>为了反向传播误差，我们所要做的就是loss.backward()。但是需要先清除现有的梯度。</p><p>Now we shall call <code>loss.backward()</code>, and have a look at conv1&#39;s bias gradients before and after the backward.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure><pre><code>conv1.bias.grad before backwardNoneconv1.bias.grad after backwardtensor([-0.0133,  0.0036,  0.0017,  0.0051,  0.0013, -0.0003])</code></pre><p>Now, we have seen how to use loss functions.</p><p><strong>Read Later:</strong></p><blockquote><p>The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is <a href="https://pytorch.org/docs/nn">here</a>.</p></blockquote><p><strong>The only thing left to learn is:</strong></p><blockquote><ul><li>Updating the weights of the network</li></ul></blockquote><h1 id="Update-the-weights"><a href="#Update-the-weights" class="headerlink" title="Update the weights"></a>Update the weights</h1><p>The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):</p><figure class="highlight plaintext"><figcaption><span>.python&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure><p>We can implement this using simple Python code:</p><figure class="highlight plaintext"><figcaption><span>.python&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = 0.01</span><br><span class="line">for f in net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure><p>However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: <code>torch.optim</code> that implements all these methods. Using it is very simple:</p><p>torch.optim 来实现参数更新</p><figure class="highlight plaintext"><figcaption><span>.python&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line"># create your optimizer</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line"># in your training loop:</span><br><span class="line">optimizer.zero_grad()   # zero the gradient buffers梯度清空</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    # Does the update</span><br></pre></td></tr></table></figure><div style="background-color: #292D3E; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px"><strong>NOTE:</strong></div><div style="background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px"><p>Observe how gradient buffers had to be manually set to zero using<code>optimizer.zero_grad()</code>. This is because gradients are accumulatedas explained in the <a href="">Backprop</a> section.</p></div>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>autograd_tutorial</title>
      <link href="/2024/06/12/autograd-tutorial/"/>
      <url>/2024/06/12/autograd-tutorial/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For tips on running notebooks in Google Colab, see</span></span><br><span class="line"><span class="comment"># https://pytorch.org/tutorials/beginner/colab</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="A-Gentle-Introduction-to-torch-autograd"><a href="#A-Gentle-Introduction-to-torch-autograd" class="headerlink" title="A Gentle Introduction to torch.autograd"></a>A Gentle Introduction to <code>torch.autograd</code></h1><p><code>torch.autograd</code> is PyTorch’s automatic differentiation engine that powers neural network training. In this section, you will get a conceptual understanding of how autograd helps a neural network train.</p><h2 id="Background：神经网络"><a href="#Background：神经网络" class="headerlink" title="Background：神经网络"></a>Background：神经网络</h2><p>Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by <em>parameters</em> (consisting of weights and biases), which in PyTorch are stored in tensors.</p><p>Training a NN happens in two steps:神经网络的训练</p><p><strong>Forward Propagation</strong>: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.</p><p><strong>Backward Propagation</strong>: In backprop, the NN adjusts its parameters proportionate to the error in its guess. 在反向传播中，神经网络根据其猜测的误差按比例调整其参数。It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (<em>gradients</em>), and optimizing the parameters using gradient descent. 它通过从输出向后遍历、收集误差相对于函数参数（梯度）的导数并使用梯度下降来优化参数来实现这一点。For a more detailed walkthrough of backprop, check out this <a href="https://www.youtube.com/watch?v=tIeHLnjs5U8">video from 3Blue1Brown</a>.</p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/f766b95e-f64f-4daf-a492-7939672647a1.png" alt="f766b95e-f64f-4daf-a492-7939672647a1"></p><h2 id="Usage-in-PyTorch"><a href="#Usage-in-PyTorch" class="headerlink" title="Usage in PyTorch"></a>Usage in PyTorch</h2><p>Let&#39;s take a look at a single training step. For this example, we load a pretrained resnet18 model from <code>torchvision</code>. We create a random data tensor to represent a single image with 3 channels, and height &amp; width of 64, and its corresponding <code>label</code> initialized to some random values. Label in pretrained models has shape (1,1000).</p><p>从 torchvision 加载预训练的 resnet18 模型。我们创建一个随机数据张量来表示具有 3 个通道、高度和宽度为 64 的单个图像，并将其相应的标签初始化为一些随机值。预训练模型中的标签形状为 (1,1000)。</p><div style="background-color: #292D3E; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px"><strong>NOTE:</strong></div><div style="background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px"><p>This tutorial works only on the CPU and will not work on GPU devices (even if tensors are moved to CUDA).</p></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> resnet18, ResNet18_Weights</span><br><span class="line">model = resnet18(weights=ResNet18_Weights.DEFAULT)</span><br><span class="line">data = torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">labels = torch.rand(<span class="number">1</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>Next, we run the input data through the model through each of its layers to make a prediction. This is the <strong>forward pass</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = model(data) <span class="comment"># forward pass</span></span><br></pre></td></tr></table></figure><p>We use the model&#39;s prediction and the corresponding label to calculate the error (<code>loss</code>). The next step is to backpropagate this error through the network. Backward propagation is kicked off when we call <code>.backward()</code> on the error tensor. Autograd then calculates and stores the gradients for each model parameter in the parameter&#39;s <code>.grad</code> attribute.</p><p>我们使用模型的预测和相应的标签来计算误差（损失）。下一步是通过网络反向传播此错误。当我们对loss调用 .backward() 时，向后传播就开始了。然后，Autograd 计算每个模型参数的梯度并将其存储在参数的 .grad 属性中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = (prediction - labels).<span class="built_in">sum</span>()</span><br><span class="line">loss.backward() <span class="comment"># backward pass</span></span><br></pre></td></tr></table></figure><p>Next, we load an optimizer, in this case SGD with a learning rate of 0.01 and <a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d">momentum</a> of 0.9. We register all the parameters of the model in the optimizer.</p><p>接下来，我们加载优化器，在本例中为 SGD，学习率为 0.01，动量为 0.9。我们在优化器中注册模型的所有参数。</p><p><font color="red">Q：momentum？</font></p><p><font color="blue">A：<a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d">带动量的随机梯度下降</a></font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optim = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>Finally, we call <code>.step()</code> to initiate gradient descent. The optimizer adjusts each parameter by its gradient stored in <code>.grad</code>.</p><p>最后，我们调用 .step() 来启动梯度下降。优化器通过存储在 .grad 中的梯度来调整每个参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optim.step() <span class="comment">#gradient descent</span></span><br></pre></td></tr></table></figure><p>At this point, you have everything you need to train your neural network. The below sections detail the workings of autograd - feel free to skip them.</p><p>以下部分详细介绍了 autograd 的工作原理。</p><hr><h1 id="Differentiation-in-Autograd"><a href="#Differentiation-in-Autograd" class="headerlink" title="Differentiation in Autograd"></a>Differentiation in Autograd</h1><p>Let&#39;s take a look at how <code>autograd</code> collects gradients. We create two tensors <code>a</code> and <code>b</code> with <code>requires_grad=True</code>. This signals to <code>autograd</code> that every operation on them should be tracked.</p><p><code>requires_grad=True</code>标志着tensor会在backward时自动微分，并将梯度记录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.tensor([<span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor([<span class="number">6.</span>, <span class="number">4.</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>We create another tensor <code>Q</code> from <code>a</code> and <code>b</code>.</p><p>$$<br>Q &#x3D; 3a^3 - b^2<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q = <span class="number">3</span>*a**<span class="number">3</span> - b**<span class="number">2</span></span><br></pre></td></tr></table></figure><p>Let&#39;s assume <code>a</code> and <code>b</code> to be parameters of an NN, and <code>Q</code> to be the error. In NN training, we want gradients of the error w.r.t. parameters, i.e.</p><p>假设<code>a</code> and <code>b</code> 是神经网络的参数，<code>Q</code>为error，则梯度计算如下：</p><p>$$<br>\frac{\partial Q}{\partial a} &#x3D; 9a^2<br>$$</p><p>$$<br>\frac{\partial Q}{\partial b} &#x3D; -2b<br>$$</p><p>When we call <code>.backward()</code> on <code>Q</code>, autograd calculates these gradients and stores them in the respective tensors&#39; <code>.grad</code> attribute.</p><p>在<code>Q</code>上调用<code>.backward()</code>时，autograd计算上述梯度并将其存储到参数的grad属性中。</p><p>We need to explicitly pass a <code>gradient</code> argument in <code>Q.backward()</code> because it is a vector. <code>gradient</code> is a tensor of the same shape as <code>Q</code>, and it represents the gradient of Q w.r.t. itself, i.e.</p><p>我们需要在 Q.backward() 中显式传递梯度参数，因为它是一个向量。<code>gradient</code> 是与 <code>Q</code> 形状相同的张量， 它代表 Q 相对于自身的梯度。</p><p>$$<br>\frac{dQ}{dQ} &#x3D; 1<br>$$</p><p>Equivalently, we can also aggregate Q into a scalar and call backward implicitly, like <code>Q.sum().backward()</code>.<br>同样，我们也可以将 Q 聚合为标量并隐式调用backward。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">external_grad = torch.tensor([<span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line">Q.backward(gradient=external_grad)</span><br></pre></td></tr></table></figure><p>Gradients are now deposited in <code>a.grad</code> and <code>b.grad</code><br>梯度现在存放在“a.grad”和“b.grad”中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># check if collected gradients are correct</span></span><br><span class="line"><span class="built_in">print</span>(<span class="number">9</span>*a**<span class="number">2</span> == a.grad)</span><br><span class="line"><span class="built_in">print</span>(-<span class="number">2</span>*b == b.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor([True, True])tensor([True, True])</code></pre><h1 id="Optional-Reading-Vector-Calculus-using-autograd"><a href="#Optional-Reading-Vector-Calculus-using-autograd" class="headerlink" title="Optional Reading - Vector Calculus using autograd"></a>Optional Reading - Vector Calculus using <code>autograd</code></h1><p>Mathematically, if you have a vector valued function $\vec{y}&#x3D;f(\vec{x})$, then the gradient of $\vec{y}$ with respect to $\vec{x}$ is a Jacobian matrix $J$:输入、输出都为向量时，梯度的计算</p><h1 id="begin-aligned-J"><a href="#begin-aligned-J" class="headerlink" title="$$\begin{aligned}J"></a>$$<br>\begin{aligned}<br>J</h1><h1 id="left-begin-array-cc-frac-partial-bf-y-partial-x-1-…-frac-partial-bf-y-partial-x-n-end-array-right"><a href="#left-begin-array-cc-frac-partial-bf-y-partial-x-1-…-frac-partial-bf-y-partial-x-n-end-array-right" class="headerlink" title="\left(\begin{array}{cc}\frac{\partial \bf{y}}{\partial x_{1}} &amp;… &amp;\frac{\partial \bf{y}}{\partial x_{n}}\end{array}\right)"></a>\left(\begin{array}{cc}<br>\frac{\partial \bf{y}}{\partial x_{1}} &amp;<br>… &amp;<br>\frac{\partial \bf{y}}{\partial x_{n}}<br>\end{array}\right)</h1><p>\left(\begin{array}{ccc}<br>\frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\<br>\vdots &amp; \ddots &amp; \vdots\<br>\frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>\end{array}\right)<br>\end{aligned}<br>$$</p><p>Generally speaking, <code>torch.autograd</code> is an engine for computing vector-Jacobian product. That is, given any vector $\vec{v}$, compute the product $J^{T}\cdot \vec{v}$</p><p>If $\vec{v}$ happens to be the gradient of a scalar function $l&#x3D;g\left(\vec{y}\right)$:</p><h1 id="vec-v"><a href="#vec-v" class="headerlink" title="$$\vec{v}"></a>$$<br>\vec{v}</h1><p>\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}<br>$$</p><p>then by the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\vec{x}$:</p><p>$$<br>\begin{aligned}<br>J^{T}\cdot \vec{v}&#x3D;\left(\begin{array}{ccc}<br>\frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{1}}\<br>\vdots &amp; \ddots &amp; \vdots\<br>\frac{\partial y_{1}}{\partial x_{n}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>\end{array}\right)\left(\begin{array}{c}<br>\frac{\partial l}{\partial y_{1}}\<br>\vdots\<br>\frac{\partial l}{\partial y_{m}}<br>\end{array}\right)&#x3D;\left(\begin{array}{c}<br>\frac{\partial l}{\partial x_{1}}\<br>\vdots\<br>\frac{\partial l}{\partial x_{n}}<br>\end{array}\right)<br>\end{aligned}<br>$$</p><p>This characteristic of vector-Jacobian product is what we use in the above example; <code>external_grad</code> represents $\vec{v}$.</p><h1 id="Computational-Graph：计算图"><a href="#Computational-Graph：计算图" class="headerlink" title="Computational Graph：计算图"></a>Computational Graph：计算图</h1><p>Conceptually, autograd keeps a record of data (tensors) &amp; all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function">Function</a> objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.</p><p><font color = "#ECB176">从概念上讲，autograd 在由 Function 对象组成的有向无环图 (DAG) 中保存数据（张量）和所有执行的操作（以及生成的新张量）的记录。在这个 DAG 中，叶子是输入张量，根是输出张量。通过从根到叶追踪该图，您可以使用链式法则自动计算梯度。</font></p><p>In a forward pass, autograd does two things simultaneously:</p><ul><li>run the requested operation to compute a resulting tensor, and</li><li>maintain the operation’s <em>gradient function</em> in the DAG.</li></ul><p><font color = "#ECB176">在前向传递中，autograd 同时执行两件事： 运行请求的操作来计算结果张量，并且 在 DAG 中维护操作的梯度函数。</font></p><p>The backward pass kicks off when <code>.backward()</code> is called on the DAG root. <code>autograd</code> then:</p><ul><li>computes the gradients from each <code>.grad_fn</code>,</li><li>accumulates them in the respective tensor’s <code>.grad</code> attribute, and</li><li>using the chain rule, propagates all the way to the leaf tensors.</li></ul><p><font color = "#ECB176">当在 DAG 根上调用 .backward() 时，向后传递开始。然后自动毕业： 计算每个 .grad_fn 的梯度， 将它们累积到各自张量的 .grad 属性中，并且 使用链式法则，一直传播到叶张量。</font></p><p>Below is a visual representation of the DAG in our example. In the graph, the arrows are in the direction of the forward pass. The nodes represent the backward functions of each operation in the forward pass. The leaf nodes in blue represent our leaf tensors <code>a</code> and <code>b</code>.</p><p><img src="https://pytorch.org/tutorials/_static/img/dag_autograd.png"></p><p><font color = "#ECB176">下面是我们示例中 DAG 的直观表示。图中，箭头指向前向传播的方向。节点代表前向传播中每个操作的backward函数。蓝色的叶节点代表我们的叶张量 a 和 b。</font></p><div style="background-color: #292D3E; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px"><strong>NOTE:</strong></div><div style="background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px"><p>An important thing to note is that the graph is recreated from scratch; after each<code>.backward()</code> call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model;you can change the shape, size and operations at every iteration if needed.</p></div><p><font color = "#ECB176">需要注意的重要一点是，该图是从头创建的；每次 .backward() 调用后，autograd 开始填充新图表。这正是允许您在模型中使用控制流语句的原因；如果需要，您可以在每次迭代时更改形状、大小和操作。</font></p><h2 id="Exclusion-from-the-DAG"><a href="#Exclusion-from-the-DAG" class="headerlink" title="Exclusion from the DAG"></a>Exclusion from the DAG</h2><p><code>torch.autograd</code> tracks operations on all tensors which have their <code>requires_grad</code> flag set to <code>True</code>. For tensors that don’t require gradients, setting this attribute to <code>False</code> excludes it from the gradient computation DAG.</p><p><font color = "#ECB176">torch.autograd 跟踪所有 require_grad 标志设置为 True 的张量上的操作。对于不需要梯度的张量，将此属性设置为 False 会将其从梯度计算 DAG 中排除。</font></p><p>The output tensor of an operation will require gradients even if only a single input tensor has <code>requires_grad=True</code>.</p><p><font color = "#ECB176">即使只有一个输入张量 require_grad&#x3D;True，操作的输出张量也将需要梯度。</font></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">z = torch.rand((<span class="number">5</span>, <span class="number">5</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a = x + y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does `a` require gradients?: <span class="subst">&#123;a.requires_grad&#125;</span>&quot;</span>)</span><br><span class="line">b = x + z</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Does `b` require gradients?: <span class="subst">&#123;b.requires_grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Does `a` require gradients?: FalseDoes `b` require gradients?: True</code></pre><p>In a NN, parameters that don&#39;t compute gradients are usually called <strong>frozen parameters</strong>. It is useful to &quot;freeze&quot; part of your model if you know in advance that you won&#39;t need the gradients of those parameters (this offers some performance benefits by reducing autograd computations).</p><p>在神经网络中，不计算梯度的参数通常称为冻结参数。如果您事先知道不需要这些参数的梯度，那么“冻结”模型的一部分很有用（这通过减少自动梯度计算提供了一些性能优势）。</p><p>In finetuning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels. Let&#39;s walk through a small example to demonstrate this. As before, we load a pretrained resnet18 model, and freeze all the parameters.</p><p>在微调中，我们冻结大部分模型，通常只修改分类器层以对新标签进行预测。让我们通过一个小例子来演示这一点。和之前一样，我们加载预训练的 resnet18 模型，并冻结所有参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line">model = resnet18(weights=ResNet18_Weights.DEFAULT)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Freeze all the parameters in the network</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></table></figure><p>Let&#39;s say we want to finetune the model on a new dataset with 10 labels. In resnet, the classifier is the last linear layer <code>model.fc</code>. We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier.</p><p>假设我们想要在具有 10 个标签的新数据集上微调模型。在resnet中，分类器是最后一个线性层model.fc。我们可以简单地用一个新的线性层（默认情况下未冻结）替换它作为我们的分类器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fc = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><p>Now all parameters in the model, except the parameters of <code>model.fc</code>, are frozen. The only parameters that compute gradients are the weights and bias of <code>model.fc</code>.</p><p>现在模型中的所有参数（除了 model.fc 的参数）都被冻结。计算梯度的唯一参数是 model.fc 的 weights<br>and bias。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Optimize only the classifier</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><p>Notice although we register all the parameters in the optimizer, the only parameters that are computing gradients (and hence updated in gradient descent) are the weights and bias of the classifier.</p><p>The same exclusionary functionality is available as a context manager in <a href="https://pytorch.org/docs/stable/generated/torch.no_grad.html">torch.no_grad()</a><br>相同的排除功能可用作 torch.no_grad() 中的上下文管理器</p><hr><h1 id="Further-readings"><a href="#Further-readings" class="headerlink" title="Further readings:"></a>Further readings:</h1><ul><li><a href="https://pytorch.org/docs/stable/notes/autograd.html">In-place operations &amp; Multithreaded Autograd</a></li><li><a href="https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC">Example implementation of reverse-mode autodiff</a></li><li><a href="https://www.youtube.com/watch?v=MswxJw-8PvE">Video: PyTorch Autograd Explained - In-depth Tutorial</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensor Tutorial</title>
      <link href="/2024/06/12/tensor-tutorial/"/>
      <url>/2024/06/12/tensor-tutorial/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For tips on running notebooks in Google Colab, see</span></span><br><span class="line"><span class="comment"># https://pytorch.org/tutorials/beginner/colab</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><p>Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.</p><p>Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing. If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along in this quick API walkthrough.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h1 id="Tensor-Initialization：初始化方式"><a href="#Tensor-Initialization：初始化方式" class="headerlink" title="Tensor Initialization：初始化方式"></a>Tensor Initialization：初始化方式</h1><p>Tensors can be initialized in various ways. Take a look at the following examples:</p><p><strong>Directly from data</strong>：使用数据直接初始化torch.tensor</p><p>Tensors can be created directly from data. The data type is automatically inferred.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]</span><br><span class="line">x_data = torch.tensor(data)</span><br></pre></td></tr></table></figure><p><strong>From a NumPy array</strong>：使用numpy的array进行初始化torch.from_numpy</p><p>Tensors can be created from NumPy arrays (and vice versa - see <code>bridge-to-np-label</code>{.interpreted-text role&#x3D;”ref”}).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.array(data)</span><br><span class="line">x_np = torch.from_numpy(np_array)</span><br></pre></td></tr></table></figure><p><strong>From another tensor</strong>：使用tensor进行初始化</p><p>The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x_ones = torch.ones_like(x_data) <span class="comment"># retains the properties of x_data（torch.ones_like形状一致的单位tensor）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;x_ones&#125;</span> \n&quot;</span>)</span><br><span class="line"></span><br><span class="line">x_rand = torch.rand_like(x_data, dtype=torch.<span class="built_in">float</span>) <span class="comment"># overrides the datatype of x_data（torch.rand_like形状一致的随机tensor）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;x_rand&#125;</span> \n&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Ones Tensor:  tensor([[1, 1],        [1, 1]]) Random Tensor:  tensor([[0.5875, 0.9007],        [0.6026, 0.1032]]) </code></pre><p>​</p><p><strong>With random or constant values</strong>：使用变量或常量指定tensor形状（shape描述tensor的形状）</p><p><code>shape</code> is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">shape = (<span class="number">2</span>, <span class="number">3</span>,)<span class="comment"># 注意元组使用圆括号</span></span><br><span class="line">rand_tensor = torch.rand(shape)</span><br><span class="line">ones_tensor = torch.ones(shape)</span><br><span class="line">zeros_tensor = torch.zeros(shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Random Tensor: \n <span class="subst">&#123;rand_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Ones Tensor: \n <span class="subst">&#123;ones_tensor&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Zeros Tensor: \n <span class="subst">&#123;zeros_tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Random Tensor:  tensor([[0.9403, 0.6379, 0.9612],        [0.6934, 0.6707, 0.0477]]) Ones Tensor:  tensor([[1., 1., 1.],        [1., 1., 1.]]) Zeros Tensor:  tensor([[0., 0., 0.],        [0., 0., 0.]])</code></pre><hr><h1 id="Tensor-Attributes：属性包括tensor的形状，元素的数据类型，tensor被存储在什么设备上"><a href="#Tensor-Attributes：属性包括tensor的形状，元素的数据类型，tensor被存储在什么设备上" class="headerlink" title="Tensor Attributes：属性包括tensor的形状，元素的数据类型，tensor被存储在什么设备上"></a>Tensor Attributes：属性包括tensor的形状，元素的数据类型，tensor被存储在什么设备上</h1><p>Tensor attributes describe their shape, datatype, and the device on which they are stored.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.rand(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Shape of tensor: <span class="subst">&#123;tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Datatype of tensor: <span class="subst">&#123;tensor.dtype&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Shape of tensor: torch.Size([3, 4])Datatype of tensor: torch.float32Device tensor is stored on: cpu</code></pre><hr><h1 id="Tensor-Operations：支持的操作（转置、索引、切片、数学操作、线性代数、随机采样等）"><a href="#Tensor-Operations：支持的操作（转置、索引、切片、数学操作、线性代数、随机采样等）" class="headerlink" title="Tensor Operations：支持的操作（转置、索引、切片、数学操作、线性代数、随机采样等）"></a>Tensor Operations：支持的操作（转置、索引、切片、数学操作、线性代数、随机采样等）</h1><p>Over 100 tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random sampling, and more are comprehensively described <a href="https://pytorch.org/docs/stable/torch.html">here</a>.</p><p>Each of them can be run on the GPU (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU by going to Edit &gt; Notebook Settings.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We move our tensor to the GPU if available</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  tensor = tensor.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">f&quot;Device tensor is stored on: <span class="subst">&#123;tensor.device&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Device tensor is stored on: cuda:0</code></pre><p>Try out some of the operations from the list. If you&#39;re familiar with the NumPy API, you&#39;ll find the Tensor API a breeze to use.</p><p><strong>Standard numpy-like indexing and slicing:</strong><br>跟numpy类似的索引和切片操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.ones(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">tensor[:,<span class="number">1</span>] = <span class="number">0</span><span class="comment"># 所有行的第一列的元素都设置为0</span></span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 0., 1., 1.],        [1., 0., 1., 1.],        [1., 0., 1., 1.],        [1., 0., 1., 1.]])</code></pre><p><strong>Joining tensors</strong> You can use <code>torch.cat</code> to concatenate a sequence of tensors along a given dimension. See also<a href="https://pytorch.org/docs/stable/generated/torch.stack.html">torch.stack</a>, another tensor joining op that is subtly different from <code>torch.cat</code>.</p><p>根据给定的维度，连接tensors；</p><p>另请参阅 torch.stack，这是另一个与 torch.cat 略有不同的tensor连接运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="number">1</span>)<span class="comment"># dim=1说明从列的方向连接，也就是横着拼接。如果dim=0则应该竖着拼接</span></span><br><span class="line"><span class="built_in">print</span>(t1)</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])</code></pre><p><strong>Multiplying tensors</strong></p><p>tensor乘法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This computes the element-wise product将矩阵的对应位置的元素相乘，因此要求形状一致</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tensor.mul(tensor) \n <span class="subst">&#123;tensor.mul(tensor)&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="comment"># Alternative syntax:*原理同上，将矩阵的对应位置的元素相乘，因此要求形状一致</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tensor * tensor \n <span class="subst">&#123;tensor * tensor&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor.mul(tensor)  tensor([[1., 0., 1., 1.],        [1., 0., 1., 1.],        [1., 0., 1., 1.],        [1., 0., 1., 1.]]) tensor * tensor  tensor([[1., 0., 1., 1.],        [1., 0., 1., 1.],        [1., 0., 1., 1.],        [1., 0., 1., 1.]])</code></pre><p>This computes the matrix multiplication between two tensors</p><p>tensors的矩阵乘法，注意其在形状上的要求，m1 x m2要求m1的列数和m2的行数相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tensor.matmul(tensor.T) \n <span class="subst">&#123;tensor.matmul(tensor.T)&#125;</span> \n&quot;</span>)</span><br><span class="line"><span class="comment"># Alternative syntax:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;tensor @ tensor.T \n <span class="subst">&#123;tensor @ tensor.T&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor.matmul(tensor.T)  tensor([[3., 3., 3., 3.],        [3., 3., 3., 3.],        [3., 3., 3., 3.],        [3., 3., 3., 3.]]) tensor @ tensor.T  tensor([[3., 3., 3., 3.],        [3., 3., 3., 3.],        [3., 3., 3., 3.],        [3., 3., 3., 3.]])</code></pre><p><strong>In-place operations</strong> Operations that have a <code>_</code> suffix are in-place. For example: <code>x.copy_(y)</code>, <code>x.t_()</code>, will change <code>x</code>.</p><p>有后缀_的操作会代替原值，即.前的对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tensor, <span class="string">&quot;\n&quot;</span>)</span><br><span class="line">tensor.add_(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br></pre></td></tr></table></figure><pre><code>tensor([[1., 0., 1., 1.],        [1., 0., 1., 1.],        [1., 0., 1., 1.],        [1., 0., 1., 1.]]) tensor([[6., 5., 6., 6.],        [6., 5., 6., 6.],        [6., 5., 6., 6.],        [6., 5., 6., 6.]])</code></pre><div style="background-color: #292D3E; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px"><strong>NOTE:</strong></div><div style="background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px"><p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate lossof history. Hence, their use is discouraged.带后缀_的操作会节省内存，但在计算微分时会出现问题，不推荐。</p></div><hr><h1 id="Bridge-with-NumPy"><a href="#Bridge-with-NumPy" class="headerlink" title="Bridge with NumPy"></a>Bridge with NumPy</h1><p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p><p>CPU上的tensor和Numpy的array可以共享其底层内存位置，此时改变其中一个就会改变另一个。</p><h1 id="Tensor-to-NumPy-array"><a href="#Tensor-to-NumPy-array" class="headerlink" title="Tensor to NumPy array"></a>Tensor to NumPy array</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.ones(<span class="number">5</span>)<span class="comment"># 创建一个tensor</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line">n = t.numpy()<span class="comment"># tensor转化为numpy array</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>t: tensor([1., 1., 1., 1., 1.])n: [1. 1. 1. 1. 1.]</code></pre><p>A change in the tensor reflects in the NumPy array.</p><p>上面的t和n此时共享了内存位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t.add_(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>t: tensor([2., 2., 2., 2., 2.])n: [2. 2. 2. 2. 2.]</code></pre><h1 id="NumPy-array-to-Tensor"><a href="#NumPy-array-to-Tensor" class="headerlink" title="NumPy array to Tensor"></a>NumPy array to Tensor</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n = np.ones(<span class="number">5</span>)<span class="comment"># 创建一个numpy array</span></span><br><span class="line">t = torch.from_numpy(n)<span class="comment"># numpy array 转化为 tensor</span></span><br><span class="line"><span class="comment"># 上面的t和n此时共享了内存位置</span></span><br></pre></td></tr></table></figure><p>Changes in the NumPy array reflects in the tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.add(n, <span class="number">1</span>, out=n)<span class="comment"># 两个数组对应位置的元素求和。n是第一个数组，1是第二个数组，输出存储在n中。</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;t: <span class="subst">&#123;t&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;n: <span class="subst">&#123;n&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)n: [2. 2. 2. 2. 2.]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x1 = np.arange(<span class="number">9.0</span>).reshape((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">x2 = np.arange(<span class="number">3.0</span>)</span><br><span class="line">x3 = np.add(x1, x2)<span class="comment"># np.add在x2长度跟x1不匹配时，自动扩展，变成[[0. 1. 2.],[0. 1. 2.],[0. 1. 2.]]</span></span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x3)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-----------------------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line">x1 = np.arange(<span class="number">3.0</span>)</span><br><span class="line">x2 = np.arange(<span class="number">9.0</span>).reshape((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">x3 = np.add(x1, x2)<span class="comment"># np.add在x1长度跟x2不匹配时，自动扩展，变成[[0. 1. 2.],[0. 1. 2.],[0. 1. 2.]]</span></span><br><span class="line"><span class="built_in">print</span>(x1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x3)</span><br></pre></td></tr></table></figure><pre><code>[[0. 1. 2.] [3. 4. 5.] [6. 7. 8.]][0. 1. 2.][[ 0.  2.  4.][ 3.  5.  7.][ 6.  8. 10.]]-----------------------------------[0. 1. 2.][[0. 1. 2.][3. 4. 5.][6. 7. 8.]][[ 0.  2.  4.][ 3.  5.  7.][ 6.  8. 10.]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">xx = np.random.randint(<span class="number">0</span>, <span class="number">10</span>, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>])<span class="comment"># 0,10表示随机数生成的范围</span></span><br><span class="line"><span class="built_in">print</span>(xx)</span><br></pre></td></tr></table></figure><pre><code>[[[[7 2 8 5]   [4 7 6 8]   [6 1 9 7]   [9 2 1 1]]  [[8 6 7 8]   [2 9 1 5]   [1 6 9 7]   [8 0 9 9]]]]</code></pre>]]></content>
      
      
      <categories>
          
          <category> PyTorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>周报汇总（持续更新）</title>
      <link href="/2024/06/11/%E5%91%A8%E6%8A%A5%E6%B1%87%E6%80%BB/"/>
      <url>/2024/06/11/%E5%91%A8%E6%8A%A5%E6%B1%87%E6%80%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h2><h3 id="6-8"><a href="#6-8" class="headerlink" title="6.8"></a>6.8</h3><p><a href="https://whatasmallship.github.io/2024/06/10/index2/">Principles of Large-Scale Machine Learning [Lecture1、2]</a></p><blockquote><p>Estimating the empirical risk with samples</p><ul><li><p>Empirical Risk介绍</p></li><li><p>计算加速措施：并行化、小批量梯度下降、硬件加速、近似计算</p></li><li><p>近似计算Empirical Risk的原理</p><ul><li><p>子采样</p><p>引入随机变量 Z，代表随机采样一个样本的损失值。多次独立地抽取 Z 的样本，这些样本的平均值将近似于经验风险 R<del>emp</del>。</p></li><li><p>大数定律</p><p>根据统计学原理，一组独立随机变量的平均值往往聚集在该随机变量的期望值周围。</p></li><li><p>中心极限定理</p><p>随着我们采样更多的 Z，样本平均值会越来越接近真实的期望值。</p></li><li><p>集中不等式</p><p>随机变量 Z 集中在某个取值附近的概率：马尔可夫不等式、切比雪夫不等式、<a href="https://zhuanlan.zhihu.com/p/693707616?utm_campaign=shareopn&utm_content=group3_article&utm_medium=social&utm_psn=1765879860495740928&utm_source=wechat_session">霍夫丁不等式</a>。</p></li></ul></li></ul></blockquote><h3 id="6-10"><a href="#6-10" class="headerlink" title="6.10"></a>6.10</h3><p>PyTorch</p><blockquote><ol><li>tensor_tutorial</li></ol><ul><li>初始化</li><li>属性：shape、dtype、device等</li><li>操作：转置、索引、切分等</li></ul><ol start="2"><li>autograd_tutorial</li></ol><ul><li>神经网络简介：前馈与反馈、误差与梯度计算</li><li>autograd在神经网络中的使用：backward()</li><li>autograd原理、计算图、取消autograd的方式</li></ul></blockquote><h3 id="6-11"><a href="#6-11" class="headerlink" title="6.11"></a>6.11</h3><p>PyTorch</p><blockquote><p>torch.nn使用（函数原型、参数含义、原理、实例）</p><ol><li>torch.nn.Linear</li><li>torch.nn.Conv2d</li><li>torch.nn.MaxPool2d</li></ol></blockquote><p><a href="https://whatasmallship.github.io/2024/06/11/CNN-1/">卷积神经网络</a></p><blockquote><ol><li>多层感知机在图像处理上的局限性</li><li>人类视觉原理</li><li>卷积神经网络基本原理</li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Learning-Record </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 这孩子打小就聪明 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN(1)</title>
      <link href="/2024/06/11/CNN-1/"/>
      <url>/2024/06/11/CNN-1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>多层感知机十分适合处理表格数据，其中行对应样本，列对应特征。然而对于高维感知数据（图像），这种缺少结构的网络可能会变得不实用。</p></blockquote><h4 id="图像处理的问题"><a href="#图像处理的问题" class="headerlink" title="图像处理的问题"></a>图像处理的问题</h4><ol><li><p>图像需要处理的数据量太大，导致成本很高，效率很低</p><blockquote><p>图像是由像素构成的，每个像素又是由颜色构成的。如果一张图片像素为1000×1000，每个像素有RGB三个信息表示颜色信息，则一张图片需要处理三百万个参数。</p></blockquote><blockquote><p><strong>CNN 解决的第一个问题就是「将复杂问题简化」，把大量参数降维成少量参数，再做处理。</strong>我们在大部分场景下，降维并不会影响结果。比如1000像素的图片缩小成200像素，并不影响肉眼认出来图片中是一只猫还是一只狗，机器也是如此。</p></blockquote></li><li><p>图像在数字化的过程中很难保留原有的特征，导致图像处理的准确率不高</p><blockquote><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog@master/wallpaper/98412-2019-06-12-tuxiangtx.png" alt="98412-2019-06-12-tuxiangtx"></p><p>假如有圆形是1，没有圆形是0，那么圆形的位置不同就会产生完全不同的数据表达。但是从视觉的角度来看，<strong>图像的内容（本质）并没有发生变化，只是位置发生了变化</strong>。</p></blockquote><blockquote><p><strong>CNN用类似视觉的方式保留了图像的特征，当图像做翻转，旋转或者变换位置时，它也能有效的识别出来是类似的图像。</strong></p></blockquote></li></ol><h4 id="人类视觉原理"><a href="#人类视觉原理" class="headerlink" title="人类视觉原理"></a>人类视觉原理</h4><ul><li>从原始信号摄入开始（瞳孔摄入像素 Pixels）</li><li>做初步处理（大脑皮层某些细胞发现边缘和方向）</li><li>抽象（大脑判定，眼前物体的形状，是圆形的）</li><li>进一步抽象（大脑进一步判定该物体是只气球）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog@master/wallpaper/d447a-2019-06-19-renlei-shijue2.jpg" alt="d447a-2019-06-19-renlei-shijue2"></p><h4 id="卷积神经网络基本原理"><a href="#卷积神经网络基本原理" class="headerlink" title="卷积神经网络基本原理"></a>卷积神经网络基本原理</h4><blockquote><p>典型的 CNN 由3个部分构成：</p><ol><li><strong>卷积层</strong>负责提取图像中的局部特征</li><li><strong>池化层</strong>用来大幅降低参数量级（降维）</li><li><strong>全连接层</strong>类似传统神经网络的部分，用来输出想要的结果</li></ol></blockquote><h5 id="卷积：提取特征"><a href="#卷积：提取特征" class="headerlink" title="卷积：提取特征"></a>卷积：提取特征</h5><blockquote><p>使用一个过滤器（卷积核）来过滤图像的各个小区域，从而得到这些小区域的特征值。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog@master/wallpaper/f144f-2019-06-19-juanji.gif" alt="f144f-2019-06-19-juanji"></p><p>具体应用中，往往有多个卷积核，可以认为，<font color = "blue"><strong>每个卷积核代表了一种图像模式，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核</strong>。</font></p><p>如果我们设计了6个卷积核，可以理解为：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。</p><h5 id="池化层（下采样）：数据降维，避免过拟合"><a href="#池化层（下采样）：数据降维，避免过拟合" class="headerlink" title="池化层（下采样）：数据降维，避免过拟合"></a>池化层（下采样）：数据降维，避免过拟合</h5><hr><p>下图示例中：原始图片大小为20×20，对其进行下采样，采样窗口为10×10，最终将其下采样成为一个2×2大小的特征图。</p><p>对于每个窗口，挑选出最大的数值作为结果。</p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog@master/wallpaper/3fd53-2019-06-19-chihua.gif" alt="3fd53-2019-06-19-chihua"></p><h5 id="全连接层：输出结果"><a href="#全连接层：输出结果" class="headerlink" title="全连接层：输出结果"></a>全连接层：输出结果</h5><blockquote><p>经过卷积层和池化层降维过的数据，全连接层才能”跑得动”，不然数据量太大，计算成本高，效率低下。</p></blockquote><h5 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h5><blockquote><p>典型的卷积神经网络，其结构为：</p><p>卷积层 – 池化层- 卷积层 – 池化层 – 卷积层 – 全连接层</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog@master/wallpaper/a8f0b-2019-06-19-lenet.png" alt="a8f0b-2019-06-19-lenet"></p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>卷积核是神经网络自己训练出来的，相当于一种“图像模式”，模型通过训练，得到一些特定的“图像模式”。训练好的模型通过这样的“知识”，来判断没有经过验证的图片的类型。</p>]]></content>
      
      
      <categories>
          
          <category> Basic-ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Large-Scale ML [lecture 1&#92;2]</title>
      <link href="/2024/06/10/index2/"/>
      <url>/2024/06/10/index2/</url>
      
        <content type="html"><![CDATA[<h2 id="lecture-1"><a href="#lecture-1" class="headerlink" title="lecture 1"></a>lecture 1</h2><ol><li><p>标准机器学习过程</p><ul><li>数据收集和准备：数据收集、数据清洗和预处理（处理缺失值、去除噪声、处理异常值；归一化、特征提取等）</li><li>模型选择与训练：根据问题类型选择适当的模型类型（如线性回归、决策树、神经网络等）</li><li>validation</li><li>test</li><li>deployment</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：validation和test的区别？</span><br><span class="line">A：验证阶段主要用于模型的调优，包括选择最佳的超参数和避免过拟合。测试阶段主要用于评估模型的最终性能，衡量模型在未见过的数据上的泛化能力。</span><br></pre></td></tr></table></figure></li><li><p>扩展到大数据上的难点</p><ul><li><p>实时数据</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：具体指什么？ss</span><br><span class="line">A：实时数据是指一些应用场景下的数据，数据持续到来，意味着数据集是变化的。比如物联网的感知场景、视频/广告点击率预测场景、天气预测场景。</span><br></pre></td></tr></table></figure></li><li><p>模型选择，更大范围的参数调整</p></li><li><p>大型数据集上的训练时间</p></li><li><p>对延迟、吞吐量和内存使用有要求的情况下，模型怎么推理和部署</p></li></ul></li><li><p>在扩展机器学习时，应该遵守的原则</p><blockquote><p>使用三个方向的技术：统计学、最优化、系统架构？</p></blockquote><ul><li><p>统计学：通过处理小型随机子样本，可以更轻松地处理大型数据集。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">具体指什么？</span><br></pre></td></tr></table></figure></li><li><p>最优化：将您的学习任务编写为优化问题，并通过迭代更新模型的快速算法来解决它。</p></li><li><p>并行系统与计算机架构：硬件与算法需要相互匹配。</p><p>为扩大规模利用并行和分布式系统形式的额外计算，机器学习计算特别适合专用硬件，例如 GPU</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：摩尔定理的具体内容？为什么说摩尔定律将不再适用？</span><br><span class="line">A：我们不能再指望通过等待两年让 CPU 速度提高两倍来提高性能和可扩展性</span><br></pre></td></tr></table></figure></li></ul></li><li><p>课程目标</p><ul><li>通过二次采样快速估计数据统计量</li><li>通过随机梯度下降 (SGD) 进行快速、可扩展的学习</li><li>用于改进SGD 的优化技术。小批量、动量、自适应学习率。</li><li>深度学习框架和自动微分。</li><li>模型选择和超参数优化。</li><li>并行和分布式培训。</li><li>量化、模型压缩和其他快速推理方法。</li></ul></li></ol><h2 id="lecture-2"><a href="#lecture-2" class="headerlink" title="lecture 2"></a>lecture 2</h2><p><strong>Estimating the empirical risk with samples</strong>：机器学习中的大多数错误或准确度都可以通过经验风险来捕获。</p><ol><li><p>经验风险</p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog@master/wallpaper/image-20241.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Q：函数L怎么理解？</span><br><span class="line">A：输入表示一个笛卡尔积，如果Y是一个集合，则Y×Y表示从Y中选取的两个元素组成的所有可能的有序对。也即，Y×Y是一个包含所有（y1,y2）形式的有序对的集合，其中y1和y2都是Y的元素。所以，该函数接受一个来自Y×Y集合的有序对作为输入，并输出一个实数。在二分类问题中，L为0-1损失函数；线性回归问题中，L可能是均方误差损失函数MSE。</span><br><span class="line">Q：经验风险函数跟L的关系？</span><br><span class="line">A：从上图公式看，L计算单个样本的损失，R计算所有样本损失的平均值。</span><br></pre></td></tr></table></figure></li><li><p>如何对经验风险的计算进行扩展</p><ul><li><p>与计算开销相关的因素</p><ul><li>样本总数n，成本与n成正比：通常经验风险计算不会占用额外的空间，因为只是计算一个标量（损失的平均值），但在某些情况下，可能需要存储中间结果或梯度信息。</li><li>损失函数L的计算成本：损失函数的复杂度也会影响计算经验风险的效率。简单的损失函数（如均方误差）计算速度较快，而复杂的损失函数（如对比损失或自定义损失函数）可能需要更多的计算资源。</li><li>评估预测器h的成本：模型的复杂度也会影响计算经验风险的效率。复杂模型（如深度神经网络）通常需要更多的计算资源来计算每个样本的预测结果。</li></ul></li><li><p>措施</p><ul><li><p>通过并行化来提高计算效率</p><blockquote><p>数据并行化：数据集划分，在多个计算节点上并行计算每个块的损失，最后合并结果</p><p>模型并行化：模型的不同部分在不同的计算资源上并行计算</p></blockquote></li><li><p>小批量提速下降：每次只计算一个小批量数据的损失</p></li><li><p>硬件加速：利用 GPU 或 TPU 等硬件加速器来并行计算</p></li><li><p>近似计算：使用subsampling来近似计算经验风险，具体见第3点。</p></li></ul></li></ul></li><li><p>近似计算经验风险</p><ul><li>引入随机变量 Z。这个随机变量 Z 代表从经验风险的求和公式中随机采样一个元素的损失值。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608131448887.png"></p><ul><li>多次独立地抽取 Z 的样本，这些样本的平均值将近似于经验风险 R<del>emp</del></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608132251370.png"></p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608131811334.png"></p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608132503301.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：E[Z]是什么意思？</span><br><span class="line">A：表示Z的数学期望。数学期望的定义回顾在上图中。</span><br></pre></td></tr></table></figure><ul><li><p>近似计算的开销与样本总数n有关系吗？</p><p>根据统计学原理，一组独立随机变量的平均值往往聚集在该随机变量的期望值周围，可以形式化地表述为（强大数定律）：</p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608133219032.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：大数定律？</span><br><span class="line">A：描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其算术平均值就有越高的概率接近期望。大数定律主要有两种表现形式：弱大数定律和强大数定律。定律的两种形式都肯定无疑地表明，样本均值收敛于真值。具体在下图。</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608133516733.png"></p></li><li><p>平均值的分布：中心极限定理，当我们有一组独立同分布的随机变量且这些随机变量具有有限的均值和方差时，当抽样次数足够大时，这些随机变量的标准化和中心化的和将近似服从正态分布。N（0，Var(Z)）是一个正态分布，其平均值μ&#x3D;0，方差为Var(Z)。换句话说，随着我们采样更多的 Z，样本平均值会越来越接近真实的期望值 E[Z]，且这种接近的误差按照正态分布分布，其方差随着 K 增加而减小。</p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608142524378.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q：这个式子怎么形象理解？</span><br><span class="line">A：如下图（标准化后的经验风险误差的分布情况）。横轴表示标准化后的经验风险的误差值，即上图左边的式子，这是抽样平均值与真实期望值之间的差距，经过标准化处理后的值。纵轴表示概率密度，这个值表示在某个误差值附近的概率密度，反映了误差值出现在该范围内的可能性。综上所述，根据中心极限定理，当样本数量足够大时，标准化后的经验风险误差值将趋于图中的正态分布。正态分布的均值为 0，意味着在大量抽样的情况下，经验风险的近似值会围绕真实期望值波动，且平均误差为 0。正态分布的标准差由Var(Z)决定，表示误差的分散程度。标准差越大，分布越宽，意味着误差波动越大；标准差越小，分布越窄，误差波动越小。</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608143232146.png"></p></li><li><p>需要多长时间才能得到合适的近似值？</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q：不太理解，这里概率不等式的意义是什么？为了证明近似计算的正确性，还是为了应用？</span><br></pre></td></tr></table></figure><blockquote><p>浓度不等式：能够限制一个有限总和偏离其期望值的概率。</p><p>马尔可夫不等式：如果S是一个非负的随机变量，且具有有限的期望值，则对于任意a&gt;0都有<img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608144503523.png"></p><p>切比雪夫不等式：如果S是一个非负的随机变量，且具有有限的期望值，则对于任意a&gt;0都有<img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608144712705.png"></p><p>霍夫丁不等式：切比雪夫不等式的一个问题，给出的概率并不够小。虽然我们知道这些总和逐渐接近类似高斯分布的东西，但我们期望从期望值偏离一定程度的概率会随着 a 的增加呈指数级下降，因为这是高斯分布的性质。霍夫丁不等式给出了总和尾部概率的一个更紧密的界限。</p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608145854077.png"></p><p>其他不等式</p><ul><li>Azuma’s：对非独立同分布也适用</li><li>Bennett’s：结合了绝对边界和方差的信息，提供了更紧凑的界限。</li></ul></blockquote></li></ul></li><li><p>想想？</p><p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/image-20240608150354611.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q：怎么才能得到一个比较好的近似？又需要多少次抽样才能得到这种程度的近似？</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Instruction</title>
      <link href="/2024/06/01/index/"/>
      <url>/2024/06/01/index/</url>
      
        <content type="html"><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/20200303233425_fodzi.jpg"></p><div style="position: relative; padding: 30% 45%;"><iframe style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" src="https://player.bilibili.com/player.html?isOutside=true&aid=35953658&bvid=BV18t41127YH&cid=63107176&page=1&as_wide=1&high_quality=1&danmaku=0" frameborder="no" scrolling="no" allowfullscreen= "ture"></iframe></div><hr><blockquote><p>aid和bvid估计是视频id<br>cid应该是客户端id，删去也不影响链接视频播放<br>page表示是选集里的第几个视频<br>as_wide表示是否为宽屏<br>high_quality表示是否高清<br>danmaku表示是否开启弹幕<br>allowfullscreen&#x3D;”ture”允许全屏</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 杂七杂八 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 视频插入 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/06/01/hello-world/"/>
      <url>/2024/06/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
