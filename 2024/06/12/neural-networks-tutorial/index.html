<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Neural Networks Tutorial | 干点啥好呢</title><meta name="author" content="Me"><meta name="copyright" content="Me"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="123# For tips on running notebooks in Google Colab, see# https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;colab%matplotlib inline Neural Networks Neural networks can be constructed using the torch.nn package. N">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Networks Tutorial">
<meta property="og:url" content="http://example.com/2024/06/12/neural-networks-tutorial/">
<meta property="og:site_name" content="干点啥好呢">
<meta property="og:description" content="123# For tips on running notebooks in Google Colab, see# https:&#x2F;&#x2F;pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;colab%matplotlib inline Neural Networks Neural networks can be constructed using the torch.nn package. N">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg">
<meta property="article:published_time" content="2024-06-12T11:41:57.000Z">
<meta property="article:modified_time" content="2024-08-22T05:06:41.294Z">
<meta property="article:author" content="Me">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2024/06/12/neural-networks-tutorial/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Neural Networks Tutorial',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-22 13:06:41'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="干点啥好呢"><span class="site-name">干点啥好呢</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Neural Networks Tutorial</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-06-12T11:41:57.000Z" title="发表于 2024-06-12 19:41:57">2024-06-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-22T05:06:41.294Z" title="更新于 2024-08-22 13:06:41">2024-08-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/PyTorch/">PyTorch</a></span></div><div class="meta-secondline"></div></div></div><article class="post-content" id="article-container"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># For tips on running notebooks in Google Colab, see</span></span><br><span class="line"><span class="comment"># https://pytorch.org/tutorials/beginner/colab</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<h1 id="neural-networks">Neural Networks</h1>
<p>Neural networks can be constructed using the <code>torch.nn</code> package.</p>
<p>Now that you had a glimpse of <code>autograd</code>, <code>nn</code> depends on <code>autograd</code> to define models and differentiate them. An <code>nn.Module</code> contains layers, and a method <code>forward(input)</code> that returns the <code>output</code>.</p>
<p>For example, look at this network that classifies digit images:</p>
<p><img src="https://pytorch.org/tutorials/_static/img/mnist.png" alt="convnet"></p>
<p>It is a simple feed-forward network. It takes the input, feeds it through several layers one after the other, and then finally gives the output.</p>
<p>A typical training procedure for a neural network is as follows:</p>
<ul>
<li>Define the neural network that has some learnable parameters (or weights)定义神经网络</li>
<li>Iterate over a dataset of inputs在输入数据集上迭代</li>
<li>Process input through the network通过nn处理输入</li>
<li>Compute the loss (how far is the output from being correct)计算损失</li>
<li>Propagate gradients back into the network’s parameters梯度反向传播</li>
<li>Update the weights of the network, typically using a simple update rule: <code>weight = weight - learning_rate * gradient</code>更新网络参数</li>
</ul>
<h2 id="define-the-network">Define the network</h2>
<p>Let’s define this network:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution；1 个输入图像通道，6 个输出通道，5x5 方形卷积</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)<span class="comment"># 参数含义依次是：输入的通道数目、输出的通道数目、卷积核的大小（当卷积是方形的时候，只需要一个整数边长即可，卷积不是方形，要输入一个元组表示 高和宽）</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b；仿射运算</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)  <span class="comment"># 5*5 from image dimension </span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        <span class="comment"># Convolution layer C1: 1 input image channel, 6 output channels,卷积层C1：1个输入图像通道，6个输出通道，</span></span><br><span class="line">        <span class="comment"># 5x5 square convolution, it uses RELU activation function, and；5x5方形卷积，它使用RELU激活函数，并且</span></span><br><span class="line">        <span class="comment"># outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch；输出大小为 (N, 6, 28, 28) 的 Tensor，其中 N 是批次的大小</span></span><br><span class="line">        c1 = F.relu(self.conv1(<span class="built_in">input</span>))<span class="comment"># 卷积1-&gt;激活；(N, 6, 28, 28)（在每个输入通道的大小为32x32的情况下）</span></span><br><span class="line">        <span class="comment"># Subsampling layer S2: 2x2 grid, purely functional,</span></span><br><span class="line">        <span class="comment"># this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor</span></span><br><span class="line">        s2 = F.max_pool2d(c1, (<span class="number">2</span>, <span class="number">2</span>))<span class="comment"># 下采样，kernel=2 x 2；(N, 6, 14, 14)</span></span><br><span class="line">        <span class="comment"># Convolution layer C3: 6 input channels, 16 output channels,</span></span><br><span class="line">        <span class="comment"># 5x5 square convolution, it uses RELU activation function, and</span></span><br><span class="line">        <span class="comment"># outputs a (N, 16, 10, 10) Tensor</span></span><br><span class="line">        c3 = F.relu(self.conv2(s2))<span class="comment"># 卷积2-&gt;激活；(N, 16, 10, 10)</span></span><br><span class="line">        <span class="comment"># Subsampling layer S4: 2x2 grid, purely functional,</span></span><br><span class="line">        <span class="comment"># this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor</span></span><br><span class="line">        s4 = F.max_pool2d(c3, <span class="number">2</span>)<span class="comment"># 下采样，kernel=2 x 2；(N, 16, 5, 5)</span></span><br><span class="line">        <span class="comment"># Flatten operation: purely functional, outputs a (N, 400) Tensor</span></span><br><span class="line">        s4 = torch.flatten(s4, <span class="number">1</span>)<span class="comment"># 展平</span></span><br><span class="line">        <span class="comment"># Fully connected layer F5: (N, 400) Tensor input,</span></span><br><span class="line">        <span class="comment"># and outputs a (N, 120) Tensor, it uses RELU activation function</span></span><br><span class="line">        f5 = F.relu(self.fc1(s4))<span class="comment"># 线性层-&gt;激活；(N, 120)</span></span><br><span class="line">        <span class="comment"># Fully connected layer F6: (N, 120) Tensor input,</span></span><br><span class="line">        <span class="comment"># and outputs a (N, 84) Tensor, it uses RELU activation function</span></span><br><span class="line">        f6 = F.relu(self.fc2(f5))<span class="comment"># 线性层-&gt;激活；(N, 84)</span></span><br><span class="line">        <span class="comment"># Gaussian layer OUTPUT: (N, 84) Tensor input, and</span></span><br><span class="line">        <span class="comment"># outputs a (N, 10) Tensor</span></span><br><span class="line">        output = self.fc3(f6)<span class="comment"># 线性层-&gt;激活；(N, 10)</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure>
<pre><code>Net(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=400, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
</code></pre>
<p>You just have to define the <code>forward</code> function, and the <code>backward</code> function (where gradients are computed) is automatically defined for you using <code>autograd</code>. You can use any of the Tensor operations in the <code>forward</code> function.</p>
<p>只需要定义 <code>forward</code> 函数， <code>backward</code> 会再使用自动微分时自动执行。</p>
<p>The learnable parameters of a model are returned by <code>net.parameters()</code></p>
<p><code>net.parameters()</code> 会返回一个模型中所有可训练的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1&#x27;s .weight：6为输出通道数（有6个不同的卷积核），1为输入通道数，卷积核形状为5x5</span></span><br></pre></td></tr></table></figure>
<pre><code>10
torch.Size([6, 1, 5, 5])
</code></pre>
<p>由于网络中包含两个卷积层和三个线性层，每层有权重和偏置两组参数可训练，因此参数中总共有十组。<br>
Let’s try a random 32x32 input. Note: expected input size of this net (LeNet) is 32x32. To use this net on the MNIST dataset, please resize the images from the dataset to 32x32.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)<span class="comment"># batch_size=1，channels=1，w x h=32x32</span></span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br></pre></td></tr></table></figure>
<pre><code>tensor([[ 0.0659,  0.1170, -0.1006, -0.0936, -0.0479, -0.0695,  0.0936,  0.0952,
         -0.0907, -0.0092]], grad_fn=&lt;AddmmBackward0&gt;)
</code></pre>
<p>Zero the gradient buffers of all parameters and backprops with random gradients:</p>
<p>梯度清空，反向传播梯度（设置一个随机的初始梯度值，该梯度的形状大小需要和out相同）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>在 PyTorch 中，调用 backward() 方法时，通常用于计算张量相对于模型参数的梯度。当使用 out.backward() 时，PyTorch 会计算 out 对于网络中各个参数的梯度。这在处理标量输出（即一个单一值）时非常常见，因为损失函数通常返回一个标量。</p>
<p>然而，当输出 out 是一个张量而非标量时（例如 out 的形状为 [1, 10]），则需要指定一个与 out 形状相同的张量作为 backward() 的输入参数，告诉 PyTorch 每个输出分量的梯度（或者叫“权重”）是多少。这一步相当于手动指定求导的方向。</p>
<div style="background-color: #292D3E; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px"><strong>NOTE:</strong></div>
<div style="background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px">
<p><code>torch.nn</code> only supports mini-batches. The entire <code>torch.nn</code>package only supports inputs that are a mini-batch of samples, and not a single sample.For example, <code>nn.Conv2d</code> will take in a 4D Tensor of<code>n Samples x n Channels x Height x Width</code>.If you have a single sample, just use <code>input.unsqueeze(0)</code> to add a fake batch dimension.</p>
</div>
<p>在 PyTorch 中，torch.nn 模块设计用于处理小批量数据（mini-batches），而不是单个样本。这意味着很多神经网络层（如 nn.Conv2d）期望输入是一个四维张量，其形状为 (batch_size, n_channels, height, width)，其中 batch_size 是样本的数量。因此，即使你只有一个样本，你仍然需要提供一个具有批量维度的输入张量。</p>
<p>input.unsqueeze(0) 的作用是向你的输入张量 input 添加一个新的维度，表示批量大小。这确保了输入符合 torch.nn 模块的预期格式。(如：input = torch.randn(3, 32, 32)，input = input.unsqueeze(0))</p>
<p>Before proceeding further, let’s recap all the classes you’ve seen so far.</p>
<p><strong>Recap:</strong> 回顾</p>
<ul>
<li><code>torch.Tensor</code> - A <em>multi-dimensional array</em> with support for autograd operations like <code>backward()</code>. Also <em>holds the gradient</em> w.r.t. the tensor.</li>
<li><code>nn.Module</code> - Neural network module. <em>Convenient way of encapsulating parameters</em>, with helpers for moving them to GPU, exporting, loading, etc.</li>
<li><code>nn.Parameter</code> - A kind of Tensor, that is <em>automatically registered as a parameter when assigned as an attribute to a</em>  <code>Module</code>.</li>
<li><code>autograd.Function</code> - Implements <em>forward and backward definitions of an autograd operation</em>. Every <code>Tensor</code> operation creates at least a single <code>Function</code> node that connects to functions that created a <code>Tensor</code> and <em>encodes its history</em>.</li>
</ul>
<p><strong>At this point, we covered:</strong></p>
<ul>
<li>Defining a neural network：定义一个神经网络</li>
<li>Processing inputs and calling backward：处理输入，发起反馈</li>
</ul>
<p><strong>Still Left:</strong></p>
<ul>
<li>Computing the loss：计算损失</li>
<li>Updating the weights of the network：更新网络参数</li>
</ul>
<h1 id="loss-function">Loss Function</h1>
<p>A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.</p>
<p>损失函数采用（输出，目标）输入对，并计算一个值来估计输出与目标的距离。</p>
<p>There are several different <a target="_blank" rel="noopener" href="https://pytorch.org/docs/nn.html#loss-functions">loss functions</a> under the nn package . A simple loss is: <code>nn.MSELoss</code> which computes the mean-squared error between the output and the target.</p>
<p>nn 包下有几种不同的损失函数。一个简单的损失是：nn.MSELoss，它计算输出和目标之间的均方误差。</p>
<p>For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.randn(<span class="number">10</span>)  <span class="comment"># a dummy target, for example</span></span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># make it the same shape as output</span></span><br><span class="line">criterion = nn.MSELoss()<span class="comment"># 设置损失计算标准</span></span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)<span class="comment"># 计算</span></span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(0.6975, grad_fn=&lt;MseLossBackward0&gt;)
</code></pre>
<p>Now, if you follow <code>loss</code> in the backward direction, using its <code>.grad_fn</code> attribute, you will see a graph of computations that looks like this:</p>
<p>现在，如果使用 <code>loss</code> 的 .grad_fn 属性向后跟踪损失，您将看到如下所示的计算图：</p>
<figure class="highlight plaintext"><figcaption><span>.sh&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d</span><br><span class="line">      -&gt; flatten -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>
<p>So, when we call <code>loss.backward()</code>, the whole graph is differentiated w.r.t. the neural net parameters, and all Tensors in the graph that have <code>requires_grad=True</code> will have their <code>.grad</code> Tensor accumulated with the gradient.</p>
<p>因此，调用 loss.backward() 时，整个计算图相对于神经网络的参数进行求导（微分）。对于计算图中的所有张量，如果其 requires_grad 属性为 True，这些张量的 .grad 属性将累积其梯度。</p>
<p>For illustration, let us follow a few steps backward:</p>
<p>loss.grad_fn 是一个与 loss 张量相关的函数，表示计算该张量所涉及的最后一个操作（函数）的梯度函数。具体来说，grad_fn 是一个 Function 对象，它记录了创建该张量的操作，从而使得反向传播过程能够计算梯度。</p>
<p>next_functions 是一个包含后续操作的元组列表。loss.grad_fn.next_functions[0][0] 指向 MseLossBackward 的第一个输入，即 output 的 grad_fn。next_functions[0]是 MseLossBackward 的第一个输入，继续索引到[0]即其grad_fn成员。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss.grad_fn)  <span class="comment"># MSELoss（生成loss的操作）</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># Linear（生成MSELoss的操作，即MSELoss之前的操作，应该是网络最后一层）</span></span><br><span class="line"><span class="built_in">print</span>(loss.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>].next_functions[<span class="number">0</span>][<span class="number">0</span>])  <span class="comment"># ReLU（生成Linear的操作，即Linear之前的操作，应该是激活函数）</span></span><br></pre></td></tr></table></figure>
<pre><code>&lt;MseLossBackward0 object at 0x00000175696E4BB0&gt;
&lt;AddmmBackward0 object at 0x00000175696E48B0&gt;
&lt;AccumulateGrad object at 0x00000175696E4BB0&gt;
</code></pre>
<h1 id="backprop">Backprop</h1>
<p>To backpropagate the error all we have to do is to <code>loss.backward()</code>. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.</p>
<p>为了反向传播误差，我们所要做的就是loss.backward()。但是需要先清除现有的梯度。</p>
<p>Now we shall call <code>loss.backward()</code>, and have a look at conv1’s bias gradients before and after the backward.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>
<pre><code>conv1.bias.grad before backward
None
conv1.bias.grad after backward
tensor([-0.0133,  0.0036,  0.0017,  0.0051,  0.0013, -0.0003])
</code></pre>
<p>Now, we have seen how to use loss functions.</p>
<p><strong>Read Later:</strong></p>
<blockquote>
<p>The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is <a target="_blank" rel="noopener" href="https://pytorch.org/docs/nn">here</a>.</p>
</blockquote>
<p><strong>The only thing left to learn is:</strong></p>
<blockquote>
<ul>
<li>Updating the weights of the network</li>
</ul>
</blockquote>
<h1 id="update-the-weights">Update the weights</h1>
<p>The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):</p>
<figure class="highlight plaintext"><figcaption><span>.python&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure>
<p>We can implement this using simple Python code:</p>
<figure class="highlight plaintext"><figcaption><span>.python&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = 0.01</span><br><span class="line">for f in net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>
<p>However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: <code>torch.optim</code> that implements all these methods. Using it is very simple:</p>
<p>torch.optim 来实现参数更新</p>
<figure class="highlight plaintext"><figcaption><span>.python&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line"># create your optimizer</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line"># in your training loop:</span><br><span class="line">optimizer.zero_grad()   # zero the gradient buffers梯度清空</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    # Does the update</span><br></pre></td></tr></table></figure>
<div style="background-color: #292D3E; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px"><strong>NOTE:</strong></div>
<div style="background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px">
<p>Observe how gradient buffers had to be manually set to zero using<code>optimizer.zero_grad()</code>. This is because gradients are accumulatedas explained in the <a href="">Backprop</a> section.</p>
</div>
<p><code>optimizer.zero_grad()</code> 和 <code>net.zero_grad()</code> 都可以用来清零模型参数的梯度，但它们的用法和场景有所不同。</p>
<ol>
<li><code>optimizer.zero_grad()</code></li>
</ol>
<ul>
<li>
<p><strong>适用场景</strong>：一般情况下，尤其是当你使用优化器（如 <code>torch.optim.SGD</code>、<code>torch.optim.Adam</code> 等）来更新模型参数时，通常会使用 <code>optimizer.zero_grad()</code> 来清零梯度。</p>
</li>
<li>
<p><strong>原因</strong>：优化器（<code>optimizer</code>）管理着你传给它的模型参数（<code>params</code>），当你调用 <code>optimizer.zero_grad()</code> 时，实际上它会遍历所有的参数，并清除这些参数上的梯度。因此，当你使用 <code>optimizer.step()</code> 来更新参数时，确保梯度累积是从零开始的。</p>
</li>
<li>
<p><strong>推荐使用</strong>：大多数情况下，如果你使用优化器来更新参数，应该使用 <code>optimizer.zero_grad()</code>。</p>
</li>
</ul>
<ol start="2">
<li><code>net.zero_grad()</code></li>
</ol>
<ul>
<li>
<p><strong>适用场景</strong>：<code>net.zero_grad()</code> 通常用于你手动管理模型参数的情况，或者你希望直接控制模型中的所有参数的梯度，而不通过优化器。比如当你没有创建优化器，或在一些自定义的优化过程时。</p>
</li>
<li>
<p><strong>原因</strong>：<code>net</code> 是一个 <code>nn.Module</code>，它的 <code>zero_grad()</code> 方法会递归遍历这个网络中的所有模块，并将所有参数的梯度清零。因此，<code>net.zero_grad()</code> 是清除整个模型的梯度的另一种方式。</p>
</li>
<li>
<p><strong>使用场景</strong>：这种方法在你想要清除特定模型的梯度，或者不使用标准的优化器进行训练时可能会用到。</p>
</li>
</ul>
<p>关键区别</p>
<ul>
<li>
<p><strong><code>optimizer.zero_grad()</code></strong>：清除由优化器管理的参数的梯度。它适合常规的训练过程，尤其是在多模型或多个优化器的情况下，使用 <code>optimizer.zero_grad()</code> 是更好的选择，因为它只清除相关参数的梯度。</p>
</li>
<li>
<p><strong><code>net.zero_grad()</code></strong>：清除整个模型（<code>net</code>）中所有参数的梯度。它适合一些特殊情况，例如你手动更新参数，或者你不使用标准的优化器进行训练。</p>
</li>
</ul>
<p>例子</p>
<p>通常，代码中会这样写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()   <span class="comment"># 清零梯度</span></span><br><span class="line">output = net(<span class="built_in">input</span>)     <span class="comment"># 前向传播</span></span><br><span class="line">loss = criterion(output, target)  <span class="comment"># 计算损失</span></span><br><span class="line">loss.backward()         <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">optimizer.step()        <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure>
<p>这种情况下，<code>optimizer.zero_grad()</code> 会清除由 <code>optimizer</code> 管理的参数的梯度。</p>
<p>如果你没有使用优化器，而是手动更新参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()         <span class="comment"># 清零梯度</span></span><br><span class="line">output = net(<span class="built_in">input</span>)     <span class="comment"># 前向传播</span></span><br><span class="line">loss = criterion(output, target)  <span class="comment"># 计算损失</span></span><br><span class="line">loss.backward()         <span class="comment"># 反向传播计算梯度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动更新参数</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param -= learning_rate * param.grad</span><br></pre></td></tr></table></figure>
<p>在这种情况下，使用 <code>net.zero_grad()</code> 清除整个模型的梯度。</p>
<p>总结</p>
<ul>
<li>使用 <code>optimizer.zero_grad()</code> 是常见的做法，因为它只清除优化器管理的参数梯度。</li>
<li><code>net.zero_grad()</code> 是一种更广泛的方式，清除整个模型的梯度，适用于手动管理参数或不使用优化器的情况。</li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/14/Review-of-Statistics/" title="Review of Statistics"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Review of Statistics</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/12/autograd-tutorial/" title="Autograd Tutorial"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Autograd Tutorial</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/08/22/PyTorch-%E4%BC%98%E5%8C%96%E5%99%A8/" title="PyTorch:优化器"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-22</div><div class="title">PyTorch:优化器</div></div></a></div><div><a href="/2024/08/22/PyTorch-%E5%85%B6%E4%BB%96%E5%87%BD%E6%95%B0/" title="PyTorch:其他函数"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-22</div><div class="title">PyTorch:其他函数</div></div></a></div><div><a href="/2024/08/22/PyTorch-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/" title="PyTorch:损失函数"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-22</div><div class="title">PyTorch:损失函数</div></div></a></div><div><a href="/2024/08/22/PyTorch-%E6%95%B0%E6%8D%AE%E9%9B%86-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/" title="PyTorch:数据集&#x2F;数据处理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-22</div><div class="title">PyTorch:数据集&#x2F;数据处理</div></div></a></div><div><a href="/2024/08/22/PyTorch-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" title="PyTorch:激活函数"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-22</div><div class="title">PyTorch:激活函数</div></div></a></div><div><a href="/2024/06/12/autograd-tutorial/" title="Autograd Tutorial"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-12</div><div class="title">Autograd Tutorial</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/4a19190013a4b3f98d0370f6645db29.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Me</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget self_memo" id="self_memo"><div class="item-headline"><i class="https://cdn.jsdelivr.net/gh/Whatasmallship/imgbed-for-self-blog/wallpaper/microsoft_todo_2019_240px.png"></i><span>TodoList</span></div><div class="item-content"><head> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>none</title> <style> body, html { margin: 0; padding: 0; width: 100%; height: 100%; } textarea { width: 100%; height: 100%; box-sizing: border-box; /* 包括内边距和边框在内的宽度和高度计算 */ } </style> </head> <body> <textarea placeholder="今天干什么？"></textarea> </body></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#neural-networks"><span class="toc-number">1.</span> <span class="toc-text">Neural Networks</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#define-the-network"><span class="toc-number">1.1.</span> <span class="toc-text">Define the network</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#loss-function"><span class="toc-number">2.</span> <span class="toc-text">Loss Function</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#backprop"><span class="toc-number">3.</span> <span class="toc-text">Backprop</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#update-the-weights"><span class="toc-number">4.</span> <span class="toc-text">Update the weights</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 By Me</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">理论是灰色的，而生活之树长青。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23lio2nMSomBJQPuq6',
      clientSecret: '469dcc3d58b8291831d2e1810da0fe0de578b8c0',
      repo: 'comments',
      owner: 'Whatasmallship',
      admin: ['Whatasmallship'],
      id: 'bfe0607a64a7c9b46513d242d7afa073',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !true) {
    if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>